[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects Done",
    "section": "",
    "text": "Generalized Linear Models: from scratch\n\n\n\n\n\n\n\ncode\n\n\nstats\n\n\n\n\n\n\n\n\n\n\n\nAug 1, 2024\n\n\nStanley Sayianka\n\n\n\n\n\n\n  \n\n\n\n\nThe impact of the pilot-UHC: a differences-in-difference approach\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nMar 22, 2024\n\n\nStanley Sayianka\n\n\n\n\n\n\n  \n\n\n\n\nMultivariate modelling of the frequency and severity of insurance claims\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nMar 22, 2024\n\n\nStanley Sayianka & Precious Mumbi\n\n\n\n\n\n\n  \n\n\n\n\nA comparison of covariance matrix estimation methods for tangency portfolios.\n\n\n\n\n\n\n\nInvestments\n\n\nQuant-finance\n\n\n\n\n\n\n\n\n\n\n\nMar 3, 2023\n\n\nStanley Sayianka\n\n\n\n\n\n\n  \n\n\n\n\nLearning representations in high-dimensional data\n\n\nRegression and Classification\n\n\n\n\nstats\n\n\nmachine-learning\n\n\n\n\n\n\n\n\n\n\n\nNov 12, 2022\n\n\nStanley Sayianka\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/claims-modelling/index.html",
    "href": "posts/claims-modelling/index.html",
    "title": "Multivariate modelling of the frequency and severity of insurance claims",
    "section": "",
    "text": "Insurance companies are financial institutions that mitigates risk by providing protection against potential losses or damages. Policyholders pay regular premiums to an insurance company in exchange for coverage. In the event of a covered incident, they can file claims to receive compensation.Â \nThis article will focus on insurance claims experience data, from a portfolio of insurance claims.\nIn trying to understand a portfolio of insurance claims, we are concerned with two variables: the claims frequency, and claims severity.\nClaim frequency: Number of expected or actual claims which occur in a given time period.\nClaim severity: This refers to the financial cost of a claim.\n\nThe data is obtained with permission from a liability insurance company in Kenya, and a glimpse of the data is shown below:\n\n\n\n\n\nClass\nReporting date\nClaim amount\nSettlement date\n\n\n\n\nProperty\n2015-10-17\n188507674\n2015-11-14\n\n\nAccident\n2017-10-13\n47539399\n2017-10-20\n\n\nLiability\n2016-02-03\n217644779\n2016-02-03\n\n\nLiability\n2012-08-10\n128384544\n2012-08-18\n\n\nLiability\n2012-05-25\n124317697\n2012-05-27\n\n\nLiability\n2018-04-18\n87314327\n2018-06-01\n\n\n\n\n\n\n\nIn total, we have 23,057 claims. The reporting date ranges from January 2005 to 2020 December. The distribution of policy classes and a smoothed time series of monthly aggregated counts for the various policy types is shown below:\n\n\n\n\n\nThe liability claims take a larger share of the portfolio, followed by property, and accident takes the smallest share of the portfolio. There was a sharp spike in claims frequency during the period mid-2012 in liability claims, and the general trend post 2015 is a declining one in all policy types.\n\n\n\nThe claim severity distribution for the three insurance classes is shown below. Due to the highly skewed nature of the data (minimum claim: 1030, maximum claim: 21 million), a log transformation is appropriate for visualization clarity.\n\n\n\n\n\nThe accident claims have the lowest average claim amounts, while the liability claims have the highest average amounts. Overall, there is a significant overlap among the claim amounts of the three classes. The data are highly skewed and heavy tailed, with the summary statistics shown below:\n\n\n\n\n\nClass\nData\nMean\nVariance\nSkewness\nKurtosis\n\n\n\n\nAccident\nSeverity\n495483\n5.152381e+12\n11\n152\n\n\nAccident\nLog severity\n11\n4.000000e+00\n1\n3\n\n\nLiability\nSeverity\n518609\n8.383795e+12\n42\n2417\n\n\nLiability\nLog severity\n12\n2.000000e+00\n0\n4\n\n\nProperty\nSeverity\n644126\n2.519351e+13\n27\n856\n\n\nProperty\nLog severity\n12\n2.000000e+00\n0\n4\n\n\n\n\n\nThe liability data has the highest skewness and kurtosis, followed by property and finally accident claims. Property claims have the highest mean and variance among the three classes. The log-transformation has effectively reduced the skewness and kurtosis of the data, making it simpler.\n\nThe correlation matrix of the aggregated data are shown below:\n\n\n\n\n\nThere is a high correlation between the claim frequency and severity in liability insurance, as compared to accident and property insurance. There is also a higher correlation between the aggregate claim frequency and time in liability insurance as compared to the rest suggesting that claim frequency in liability insurance has been increasing with time.\n\n\n\nWe aim to model the aggregated frequency at the monthly level and the aggregated claim severity at the monthly level. Typical analyses often seperate the modelling of frequency from severity, however in this analysis, we aim to use multi-variate modelling to model both simultaneously.\nSince the claim frequencies are count data, then an appropriate statistical distribution should have the poperty that: it is a discrete distribution, has positive skewness. Such distributions include count distributions such as the Poisson distribution and the negative binomial distribution.\nFor claim severity modelling, the distributions appropriate for analysis are those which: have high skewness, support is the positive real line, have heavy tails. Examples of distributions for such include: Log-normal distribution, Pareto distribution and other heavy tailed distributions such as the Burr distribution.\nFor a start, we fit a multivariate model on the data, where the claim frequency is modelled as a poisson random variable, while the log-claim severity is modelled as a normal random variable. The model fitted is an intercept only model."
  },
  {
    "objectID": "posts/claims-modelling/index.html#claim-frequency",
    "href": "posts/claims-modelling/index.html#claim-frequency",
    "title": "Multivariate modelling of the frequency and severity of insurance claims",
    "section": "",
    "text": "In total, we have 23,057 claims. The reporting date ranges from January 2005 to 2020 December. The distribution of policy classes and a smoothed time series of monthly aggregated counts for the various policy types is shown below:\n\n\n\n\n\nThe liability claims take a larger share of the portfolio, followed by property, and accident takes the smallest share of the portfolio. There was a sharp spike in claims frequency during the period mid-2012 in liability claims, and the general trend post 2015 is a declining one in all policy types."
  },
  {
    "objectID": "posts/claims-modelling/index.html#claim-severity",
    "href": "posts/claims-modelling/index.html#claim-severity",
    "title": "Multivariate modelling of the frequency and severity of insurance claims",
    "section": "",
    "text": "The claim severity distribution for the three insurance classes is shown below. Due to the highly skewed nature of the data (minimum claim: 1030, maximum claim: 21 million), a log transformation is appropriate for visualization clarity.\n\n\n\n\n\nThe accident claims have the lowest average claim amounts, while the liability claims have the highest average amounts. Overall, there is a significant overlap among the claim amounts of the three classes. The data are highly skewed and heavy tailed, with the summary statistics shown below:\n\n\n\n\n\nClass\nData\nMean\nVariance\nSkewness\nKurtosis\n\n\n\n\nAccident\nSeverity\n495483\n5.152381e+12\n11\n152\n\n\nAccident\nLog severity\n11\n4.000000e+00\n1\n3\n\n\nLiability\nSeverity\n518609\n8.383795e+12\n42\n2417\n\n\nLiability\nLog severity\n12\n2.000000e+00\n0\n4\n\n\nProperty\nSeverity\n644126\n2.519351e+13\n27\n856\n\n\nProperty\nLog severity\n12\n2.000000e+00\n0\n4\n\n\n\n\n\nThe liability data has the highest skewness and kurtosis, followed by property and finally accident claims. Property claims have the highest mean and variance among the three classes. The log-transformation has effectively reduced the skewness and kurtosis of the data, making it simpler.\n\nThe correlation matrix of the aggregated data are shown below:\n\n\n\n\n\nThere is a high correlation between the claim frequency and severity in liability insurance, as compared to accident and property insurance. There is also a higher correlation between the aggregate claim frequency and time in liability insurance as compared to the rest suggesting that claim frequency in liability insurance has been increasing with time."
  },
  {
    "objectID": "posts/claims-modelling/index.html#modelling",
    "href": "posts/claims-modelling/index.html#modelling",
    "title": "Multivariate modelling of the frequency and severity of insurance claims",
    "section": "",
    "text": "We aim to model the aggregated frequency at the monthly level and the aggregated claim severity at the monthly level. Typical analyses often seperate the modelling of frequency from severity, however in this analysis, we aim to use multi-variate modelling to model both simultaneously.\nSince the claim frequencies are count data, then an appropriate statistical distribution should have the poperty that: it is a discrete distribution, has positive skewness. Such distributions include count distributions such as the Poisson distribution and the negative binomial distribution.\nFor claim severity modelling, the distributions appropriate for analysis are those which: have high skewness, support is the positive real line, have heavy tails. Examples of distributions for such include: Log-normal distribution, Pareto distribution and other heavy tailed distributions such as the Burr distribution.\nFor a start, we fit a multivariate model on the data, where the claim frequency is modelled as a poisson random variable, while the log-claim severity is modelled as a normal random variable. The model fitted is an intercept only model."
  },
  {
    "objectID": "posts/cov-mat-estimation-tangency-ports/index.html",
    "href": "posts/cov-mat-estimation-tangency-ports/index.html",
    "title": "A comparison of covariance matrix estimation methods for tangency portfolios.",
    "section": "",
    "text": "This article investigates the problem of covariance matrix estimation for tangency portfolios using data from the Nairobi Securities Exchange (NSE) Specifically, I compare several methods of covariance matrix estimation and evaluate their performance in constructing tangency portfolios. The results provide insights into the most effective methods for estimating the covariance matrix in the context of the NSE, which can be valuable for portfolio managers and investors seeking to optimize their investment portfolios in the NSE."
  },
  {
    "objectID": "posts/cov-mat-estimation-tangency-ports/index.html#the-efficient-frontier-and-efficient-portfolios",
    "href": "posts/cov-mat-estimation-tangency-ports/index.html#the-efficient-frontier-and-efficient-portfolios",
    "title": "A comparison of covariance matrix estimation methods for tangency portfolios.",
    "section": "The efficient frontier and efficient portfolios",
    "text": "The efficient frontier and efficient portfolios\nThe efficient frontier is a concept used in modern portfolio theory that helps investors to identify the optimal portfolio that maximizes returns for a given level of risk. The theory assumes that investors are risk-averse and will choose the portfolio with the highest expected return for a given level of risk. It is usually represented as a graph that plots the expected return and standard deviation2 of all possible portfolios that can be constructed from a set of assets.2Â as a measure of risk\nThe portfolios on the efficient frontier are considered to be efficient because they provide the highest possible expected return for a given level of risk. Thus, investors can choose any portfolio on the efficient frontier that suits their level of risk tolerance. An important aspect to note from the efficient frontier is: investors can only increase their expected return by taking on more risk."
  },
  {
    "objectID": "posts/cov-mat-estimation-tangency-ports/index.html#tangency-portfolio",
    "href": "posts/cov-mat-estimation-tangency-ports/index.html#tangency-portfolio",
    "title": "A comparison of covariance matrix estimation methods for tangency portfolios.",
    "section": "Tangency portfolio",
    "text": "Tangency portfolio\nA tangency portfolio, is a portfolio that maximizes the risk-adjusted return for a given level of risk aversion. Specifically, a tangency portfolio lies on the efficient frontier, and it is tangent to the capital market line3. It is widely used in portfolio management to construct investment portfolios that offer the highest expected return for a given level of risk or the lowest level of risk for a given expected return.3Â Simple a straight line representing the risk/reward profiles of different combinations of a risky portfolio with a riskless asset\nDespite the emergence of alternative portfolio construction techniques, such as risk parity, tangency portfolios remain relevant in investment management due to their ability to provide an optimal balance between risk and return. They are a key component of modern portfolio theory and provide a useful reference point for evaluating portfolio performance. They are constructed based on the efficient frontier - as such, they represent the most efficient way to achieve a particular level of expected return, making them an important tool for portfolio optimization.\nThe optimization problem for the tangency portfolio can be expressed follows:\nSuppose we have a universe of \\(n\\) assets with returns \\(\\mathbf{\\mu}\\) and covariance matrix \\(\\mathbf{\\Sigma}\\). We want to construct a portfolio with weights \\(\\mathbf{w}\\) that maximizes the Sharpe ratio subject to the constraint that all weights are non-negative, and sum to one:\n\\[max_w \\hspace{2 mm} h(w) = \\frac{\\mu^Tw - r_f}{w^T \\Sigma w}\\] \\[\\text{subject to} \\quad \\mathbf{w} \\geq \\mathbf{0}, \\hspace{2 mm} \\sum_{i=1}^n w_i = 1\\] The quantity \\(h(w)\\) is precisely the Sharpe ratio.\nSolving for \\(w\\), gives:\n\\[\\mathbf{w} = \\frac{\\mathbf{\\Sigma}^{-1} (\\mathbf{\\mu} - r_f \\mathbf{1})}{\\mathbf{\\mathbf{1^T}  \\Sigma}^{-1}(\\mathbf{\\mu} - r_f \\mathbf{1})} \\]\nFrom the solution above, the covariance matrix and mean vector are crucial in the solution for the tangency portfolio. The mean vector represents the expected returns for each asset in the portfolio, while the covariance matrix represents the co-movements between the returns of each asset in the portfolio.\nThe optimal portfolio is found by balancing the expected returns and risks of the individual assets in the portfolio, which is done using the mean vector and covariance matrix. In the next section, we explore several ways of estimating the expected returns and covariance matrix for the assets in the portfolio\nIn the next section, we review covariance matrix estimation methods present in literature, giving their formulations, and using them in computing optimal allocations to the investment universe. We briefly begin with the sample covariance matrix, and proceed to alternative covariance matrix estimation methods."
  },
  {
    "objectID": "posts/cov-mat-estimation-tangency-ports/index.html#the-sample-covariance-matrix-sample",
    "href": "posts/cov-mat-estimation-tangency-ports/index.html#the-sample-covariance-matrix-sample",
    "title": "A comparison of covariance matrix estimation methods for tangency portfolios.",
    "section": "The sample covariance matrix (SAMPLE)",
    "text": "The sample covariance matrix (SAMPLE)\nThe sample covariance matrix is among the earliest and commonly used estimator, especially because it arises as the maximum likelihood solution under normality. It is calculated from the historical returns of the assets in the portfolio, with the assumption that the historical returns of the assets are representative of their future returns. This assumption may be violated, especially in dynamic markets.\nThe sample location estimate for a portfolio of \\(n\\) observations and \\(p\\) assets is given by:\n\\[ E(r_p) = \\sum_{i=1}^{n} w_i E(r_i)\\] The sample covariance matrix is given by:\n\\[\\begin{equation}\n\\hat{\\Sigma} = \\frac{1}{n-1} \\sum_{i=1}^{n} (\\mathbf{x_i} - \\boldsymbol{\\bar{x}})(\\mathbf{x_i} - \\boldsymbol{\\bar{x}})^T,\n\\end{equation}\\]\nwhere:\n\\(E(r_p)\\) is the expected return of the portfolio\n\\(E(r_i)\\) is the expected return of the \\(i\\)th asset\n\\(\\mathbf{x_i}\\) is the \\(i\\)-th observation vector,\n\\(\\boldsymbol{\\bar{x}}\\) is the sample mean vector, and the superscript \\(T\\) denotes the matrix transpose operation.\nNote that \\(\\hat{\\Sigma}\\) is a symmetric \\(p \\times p\\) matrix.\nOne of the main drawbacks of the sample covariance matrix is that it is sensitive to outliers. Additionally, if the number of assets in the portfolio is large, singularity issues may emerge when computing the sample covariance matrix.\nSince, the sample covariance matrix is sensitive to outlying observations, there is need to examine other ârobustâ approaches to estimating the covariance matrix of historical stock returns. The robust methods provide alternative approaches to classical methods for location and scale estimates, as they are not affected by slight departures from model assumptions.\nIn the next section, we investigate some of the robust covariance estimators:"
  },
  {
    "objectID": "posts/cov-mat-estimation-tangency-ports/index.html#minimum-covariance-determinant-estimator-mcd",
    "href": "posts/cov-mat-estimation-tangency-ports/index.html#minimum-covariance-determinant-estimator-mcd",
    "title": "A comparison of covariance matrix estimation methods for tangency portfolios.",
    "section": "1. Minimum Covariance Determinant Estimator (MCD)",
    "text": "1. Minimum Covariance Determinant Estimator (MCD)\nThe MCD estimator is based on the concept of finding the subset of data points: \\(h(&gt;N/2)\\) whose classical covariance matrix has the lowest possible determinant, which is a measure of the spread of the data.\nWhen using the MCD estimator, the location estimate of a portfolio is given by the arithmetic mean of the asset returns in the robust subset, as follows:\n\\[\\bar{r}_{MCD} = \\frac{1}{n_{robust}} \\sum_{i=1}^{n_{robust}} r_i\\] Where:\n\\(\\bar{r}_{MCD}\\) is the location estimate using the MCD estimator,\n\\(n_{robust}\\) is the number of assets in the robust subset, and\n\\(r_i\\) is the return of the \\(i\\)th asset in the robust subset.\nThe MCD estimator for the covariance matrix is computed as follows:\n\\[\\begin{equation}\n\\hat{\\Sigma}_{MCD} = c_{p,n} \\cdot \\text{MAD}(\\mathbf{X})^{-2} \\cdot \\sum_{i=1}^{h} w_i (\\mathbf{x_i} - \\boldsymbol{\\bar{x}})(\\mathbf{x_i} - \\boldsymbol{\\bar{x}})^T,\n\\end{equation}\\]\nwhere:\n\\(\\mathbf{X}\\) is a \\(n \\times p\\) matrix of observations,\n\\(\\boldsymbol{\\bar{x}}\\) is the sample mean vector,\n\\(\\text{MAD}(\\mathbf{X})\\) is the Median Absolute Deviation of the observations,\n\\(h\\) is the number of non-outlier observations,\n\\(w_i\\) are weights assigned to each non-outlier observation, and\n\\(c_{p,n}\\) is a constant that depends on the dimension of the data and the number of observations.\nOne of the main strengths of the MCD estimator is its ability to handle both symmetric and asymmetric outliers in the data, making it robust. It is also computationally efficient and can be used with high dimensional datasets, and portfolios with a large number of assets.\nA limitation of the MCD estimator is that it assumes that the data points are multivariate normal, which may not always hold true in practice. The MCD estimator may also produce unreliable estimates of the covariance matrix when the proportion of outliers in the data is very high."
  },
  {
    "objectID": "posts/cov-mat-estimation-tangency-ports/index.html#minimum-volume-ellipsoid-estimator-mve",
    "href": "posts/cov-mat-estimation-tangency-ports/index.html#minimum-volume-ellipsoid-estimator-mve",
    "title": "A comparison of covariance matrix estimation methods for tangency portfolios.",
    "section": "2. Minimum Volume Ellipsoid Estimator (MVE)",
    "text": "2. Minimum Volume Ellipsoid Estimator (MVE)\nThe MVE is a multivariate location and scale estimator which uses a subset of the data in its computation. It is based on the concept of fitting an ellipsoid to the data points that minimizes its volume, while ensuring that it contains a specified proportion4 of the data points. It can be thought of as estimating the mean and covariance of the âgood partâ of the data.4Â This specified proportion is based on user-supplied quantiles\nWhen using the MVE estimator, the location estimate of a portfolio is given by the arithmetic mean of the asset returns in the ellipsoid that encloses the robust subset, as follows:\n\\[\\bar{r}_{MVE} = \\frac{1}{n_{robust}} \\sum_{i=1}^{n_{robust}} r_i\\]\nwhere:\n\\(\\bar{r}_{MVE}\\) is the location estimate using the MVE estimator,\n\\(n_{robust}\\) is the number of assets in the robust subset, and\n\\(r_i\\) is the return of the \\(i\\)th asset in the robust subset.\nThe MVE estimator for the covariance matrix is computed as follows:\n\\[\\begin{equation}\n\\hat{\\Sigma}_{MVE} = \\frac{n}{m} \\cdot \\text{cov}_{\\text{MCD}}(\\mathbf{X}),\n\\end{equation}\\]\nwhere:\n\\(\\mathbf{X}\\) is a \\(n \\times p\\) matrix of observations,\n\\(\\text{cov}_{\\text{MCD}}(\\mathbf{X})\\) is the covariance matrix computed using the MCD estimator, and\n\\(m\\) is the number of observations used to compute the MCD estimator.\nOther than being robust to outliers, the MVE provides a more accurate estimate of the covariance matrix when the distribution of the asset returns is non-normal. In such cases, the MVE estimator can capture the non-linear relationships between the assets and produce more accurate estimates of the covariance matrix.\nHowever, the MVE estimator can be computationally expensive to calculate, especially when dealing with portfolios with a large number of assets and the assumption that data points are elliptically distributed may not always hold true in some cases."
  },
  {
    "objectID": "posts/cov-mat-estimation-tangency-ports/index.html#orthogonalized-gnanadesikan-kettenring-estimator-ogk",
    "href": "posts/cov-mat-estimation-tangency-ports/index.html#orthogonalized-gnanadesikan-kettenring-estimator-ogk",
    "title": "A comparison of covariance matrix estimation methods for tangency portfolios.",
    "section": "3. Orthogonalized Gnanadesikan-Kettenring Estimator (OGK)",
    "text": "3. Orthogonalized Gnanadesikan-Kettenring Estimator (OGK)\nThis method is based on the concept of orthogonalizing the data matrix to remove any correlations between the variables and then estimating the covariance matrix from the resulting matrix.\nThe OGK estimate for the robust location measure is defined as:\n\\[\\mu(X) = \\frac{\\sum_i w_ix_i}{\\sum_i w_i}\\]\nWhere,\n\\(w_i\\) are the robustness weights compute as:\n\\[w_i := w_{c1}\\frac{(x_i - med(X))}{MAD(X)}\\]\nThe OGK estimator for the covariance matrix can be computed as follows:\n\\[\\begin{equation}\n\\hat{\\Sigma}_{OGK} = \\frac{1}{n} \\cdot \\sum_{i=1}^{n} \\lambda_i(\\mathbf{S}) \\cdot \\mathbf{V_i} \\cdot \\mathbf{V_i}^T,\n\\end{equation}\\]\nwhere\n\\(\\mathbf{S}\\): is the sample covariance matrix,\n\\(\\lambda_i(\\mathbf{S})\\) and \\(\\mathbf{V_i}\\) are the eigenvalues and eigenvectors of \\(\\mathbf{S}\\), respectively, and \\(n\\) is the number of observations.\nOne of the main strengths of the OGK estimator is its ability to handle datasets with high levels of multicollinearity5, which can cause instability in the estimates of the covariance matrix. The OGK estimator reduces this instability by orthogonalizing the data matrix5Â High correlations between the variables in the data\nThe OGK estimator also produces estimates of the covariance matrix that are more accurate than those obtained using the sample covariance matrix when the distribution of the asset returns is non-normal.\nSome limitations to using the OGK estimator include: it can be computationally expensive to calculate, especially when dealing with portfolios with a large number of assets. The OGK estimator may also produce unreliable estimates of the covariance matrix when the proportion of outliers in the data is very high."
  },
  {
    "objectID": "posts/cov-mat-estimation-tangency-ports/index.html#nearest-neighbour-variance-estimator-nnv",
    "href": "posts/cov-mat-estimation-tangency-ports/index.html#nearest-neighbour-variance-estimator-nnv",
    "title": "A comparison of covariance matrix estimation methods for tangency portfolios.",
    "section": "4. Nearest-Neighbour Variance Estimator (NNV)",
    "text": "4. Nearest-Neighbour Variance Estimator (NNV)\nThe NNV estimator is based on the concept of using the k-nearest neighbors of each data point to estimate its variance, which is then used to estimate the covariance matrix.\nThe location estimate is given by the arithmetic mean of the asset returns in the dataset. The formula for the location estimate using the NNV estimator is:\n\\[\\bar{r}_{NNV} = \\frac{1}{n} \\sum_{i=1}^{n} r_i\\]\nwhere:\n\\(\\bar{r}_{NNV}\\) is the location estimate using the NNV estimator,\n\\(n\\) is the number of assets in the portfolio,\nand \\(r_i\\) is the return of the \\(i^{th}\\) asset in the portfolio.\nThe NNV estimator for the covariance matrix is computed as follows:\n\\[\\begin{equation}\n\\hat{\\Sigma}_{NNV} = \\frac{2}{n(n-1)h^2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} w\\left(\\frac{|\\mathbf{x_i} - \\mathbf{x_j}|}{h}\\right) (\\mathbf{x_i} - \\mathbf{\\bar{x}})(\\mathbf{x_j} - \\mathbf{\\bar{x}})^T,\n\\end{equation}\\]\nwhere\n\\(\\mathbf{x_i}\\) and \\(\\mathbf{x_j}\\) are the \\(i^{th}\\) and \\(j^{th}\\) observations, respectively,\n\\(\\mathbf{\\bar{x}}\\) is the sample mean,\n\\(h\\) is a smoothing parameter, and \\(w(\\cdot)\\) is a weighting function6.6Â A common choice for \\(w(\\cdot)\\) is the Epanechnikov kernel.\nOne of the main strengths of the NNV estimator is its ability to handle datasets with a large number of assets, and thus, it can produce estimates of the covariance matrix that are less affected by the curse of dimensionality than other methods.\nLimitations to using the NNV estimator include: it assumes that the data points are independent and identically distributed, which may not always hold true in practice. Additionally, the NNV estimator may produce unreliable estimates of the covariance matrix when the dataset contains outliers or when the nearest neighbors parameter \\(k\\) is chosen incorrectly."
  },
  {
    "objectID": "posts/cov-mat-estimation-tangency-ports/index.html#shrinkage-estimator-shrink",
    "href": "posts/cov-mat-estimation-tangency-ports/index.html#shrinkage-estimator-shrink",
    "title": "A comparison of covariance matrix estimation methods for tangency portfolios.",
    "section": "5. Shrinkage Estimator (SHRINK)",
    "text": "5. Shrinkage Estimator (SHRINK)\nThe Shrinkage estimator is based on the concept of shrinking the sample covariance matrix towards a target matrix such as the diagonal matrix or a structured matrix, which can improve the accuracy of the estimated covariance matrix.\nThe Shrinkage Estimator of the location parameter is given by the arithmetic mean of the asset returns.\nThe Shrinkage Estimator for the covariance matrix is computed as follows:\n\\[\\begin{equation}\n\\hat{\\Sigma}_{Shrink} = \\alpha \\cdot \\hat{\\Sigma}_{D} + (1-\\alpha) \\cdot \\hat{\\Sigma}_{S},\n\\end{equation}\\]\nwhere:\n\\(\\hat{\\Sigma}_{D}\\) is the diagonal matrix of the sample variances,\n\\(\\hat{\\Sigma}_{S}\\) is the target matrix, and \\(\\alpha\\) is the shrinkage intensity parameter that determines the degree of shrinkage towards the target matrix.\nA common choice for the target matrix is the Ledoit-Wolf estimator, which is based on the assumption that the true covariance matrix can be expressed as a linear combination of a diagonal matrix and a low-rank matrix. The Ledoit-Wolf estimator is computed as follows:\n\\[\\begin{equation}\n\\hat{\\Sigma}_{LW} = \\gamma \\cdot \\hat{\\Sigma}_{D} + (1-\\gamma) \\cdot \\hat{\\Sigma}_{L},\n\\end{equation}\\]\nwhere:\n\\(\\hat{\\Sigma}_{D}\\) is the diagonal matrix of the sample variances,\n\\(\\hat{\\Sigma}_{L}\\) is the low-rank matrix that minimizes the Frobenius norm distance to the sample covariance matrix, and\n\\(\\gamma\\) is a parameter that determines the degree of shrinkage towards the diagonal matrix.\nThe main strengths of the Shrinkage estimator is that it produces reliable estimates of the covariance matrix than the sample covariance matrix when the number of observations or assets is small, as well as when the number of assets is large, and thus, it is less affected by the curse of dimensionality than other methods.\nHowever, there are also some limitations to using the Shrinkage estimator. The accuracy of the estimated covariance matrix depends on the choice of the target matrix. It is also important to note that, the Shrinkage estimator may produce unreliable estimates of the covariance matrix when the dataset contains outliers or when the amount of shrinkage is too large."
  },
  {
    "objectID": "posts/cov-mat-estimation-tangency-ports/index.html#bagging-estimator-bagged",
    "href": "posts/cov-mat-estimation-tangency-ports/index.html#bagging-estimator-bagged",
    "title": "A comparison of covariance matrix estimation methods for tangency portfolios.",
    "section": "6. Bagging Estimator (BAGGED)",
    "text": "6. Bagging Estimator (BAGGED)\nThe bagged estimator is based on the concept of bootstrapping, which involves creating multiple samples of the original dataset and using these samples to estimate the covariance matrix.\nThe Bagging Estimator for the location parameter is then given by the average of the sample means:\n\\[\n\\boldsymbol{\\tilde{r}} = \\frac{1}{B} \\sum_{b=1}^B \\boldsymbol{\\bar{r}}_b\n\\]\nwhere\n\\(\\boldsymbol{\\bar{r}}_b\\) is the sample mean of the \\(b\\)th bootstrap sample, and\n\\(B\\) is the number of bootstrap samples.\nThe Bagging Estimator for the covariance matrix is computed as follows:\n\\[\\begin{equation}\n\\hat{\\Sigma}_{Bagging} = \\frac{1}{B} \\sum_{b=1}^{B} \\hat{\\Sigma}_{b},\n\\end{equation}\\]\nwhere \\(\\hat{\\Sigma}_{b}\\) is the sample covariance matrix computed from the \\(b^{th}\\) bootstrap sample, and \\(B\\) is the number of bootstrap samples used.\nTo generate each bootstrap sample, we randomly sample \\(n\\) observations with replacement from the original data. Then, we compute the sample covariance matrix from the bootstrap sample.\nAn advantage of the bagged estimator is: that it can handle datasets with non-normal distributions or with a high proportion of outliers. By creating multiple samples of the dataset, it can reduce the impact of extreme values on the estimated covariance matrix.\nHowever, the accuracy of the estimated covariance matrix depends on the number of bootstrap samples (\\(B\\)) and the Bagged estimator may also produce unreliable estimates of the covariance matrix when the dataset contains highly correlated assets."
  },
  {
    "objectID": "posts/cov-mat-estimation-tangency-ports/index.html#multivariate-students-t-estimator-cov-t",
    "href": "posts/cov-mat-estimation-tangency-ports/index.html#multivariate-students-t-estimator-cov-t",
    "title": "A comparison of covariance matrix estimation methods for tangency portfolios.",
    "section": "7. Multivariate Studentâs t estimator (COV-T)",
    "text": "7. Multivariate Studentâs t estimator (COV-T)\nThe Multivariate Studentâs t estimator is a technique used to estimate the covariance matrix in portfolio optimization based on the assumption that the asset returns follow a multivariate Studentâs t distribution, which can better capture the presence of outliers and heavy tails in the data compared to the normal distribution.\nThe Multivariate Studentâs t Estimator for the location parameter is given by:\n\\[\n\\boldsymbol{\\tilde{r}} = \\frac{\\sum_{i=1}^n w_i \\boldsymbol{r}_i}{\\sum_{i=1}^n w_i}\n\\]\nwhere:\n\\(\\boldsymbol{r}_i\\) is the \\(i^{th}\\) asset return vector, and\n\\(w_i\\) is the weight assigned to the \\(i^{th}\\) asset, which is determined by the Mahalanobis distance of the \\(i^{th}\\) return vector from the empirical median of the returns, and the degrees of freedom parameter \\(\\nu\\) of the Studentâs t distribution.\nThe Multivariate Studentâs t estimator for the covariance matrix is computed as follows:\n\\[\\begin{equation}\n\\hat{\\Sigma}_{t} = \\frac{\\kappa}{\\kappa-2} \\cdot \\frac{\\sum_{i=1}^{n}(X_i-\\bar{X})(X_i-\\bar{X})^T}{n},\n\\end{equation}\\]\nwhere:\n\\(X_i\\) is the \\(i^{th}\\) observation in the data,\n\\(\\bar{X}\\) is the sample mean,\n\\(n\\) is the sample size, and\n\\(\\kappa\\) is a parameter that controls the degrees of freedom of the Studentâs t-distribution.\nThe main strength of the Multivariate Studentâs t estimator is that it can produce more reliable estimates of the covariance matrix than other methods when the dataset contains extreme values or when the returns are not normally distributed.\nA key limitation to this method is that it assumes that the asset returns follow a multivariate Studentâs t distribution, which may not always hold true in practice. Additionally, the Multivariate Studentâs t estimator may produce unreliable estimates of the covariance matrix when the dataset contains highly correlated assets."
  },
  {
    "objectID": "posts/cov-mat-estimation-tangency-ports/index.html#adaptive-re-weighted-estimator-arw",
    "href": "posts/cov-mat-estimation-tangency-ports/index.html#adaptive-re-weighted-estimator-arw",
    "title": "A comparison of covariance matrix estimation methods for tangency portfolios.",
    "section": "8. Adaptive Re-weighted estimator (ARW)",
    "text": "8. Adaptive Re-weighted estimator (ARW)\nThe Adaptive Re-weighted estimator is a technique based on the idea of iteratively re-weighting the observations based on their Mahalanobis distances from the current estimate of the mean and covariance matrix, which can improve the accuracy of the estimated covariance matrix. It is designed to be robust to outliers and heavy-tailed data.\n\\[\\boldsymbol{\\tilde{r}} = \\boldsymbol{m}_{AR}^{(K)} = \\boldsymbol{m}_{AR}^{(0)} + \\sum_{k=1}^K \\boldsymbol{\\epsilon}^{(k)} \\boldsymbol{W}^{(k)}\\]\nwhere:\n\\(\\boldsymbol{m}_{AR}^{(0)}\\) is an initial estimate of the location parameter,\n\\(\\boldsymbol{\\epsilon}^{(k)}\\) is the residual error of the linear regression of the asset returns on the estimate of the location parameter at iteration \\(k-1\\), and\n\\(\\boldsymbol{W}^{(k)}\\) is the weight matrix assigned to each asset at iteration \\(k\\), which is determined by the Mahalanobis distance of the return vector from the estimate of the location parameter at iteration \\(k-1\\), and a tuning constant \\(\\alpha_k\\) that scales the distance threshold.\nThe re-weighted estimator for the covariance matrix is given by:\n\\[\\begin{equation}\n\\hat{\\Sigma}_{ARW} = \\frac{1}{n} \\sum_{i=1}^{n} w_i (X_i - \\bar{X})(X_i - \\bar{X})^T,\n\\end{equation}\\]\nwhere:\n\\(X_i\\) is the \\(i^{th}\\) observation in the data,\n\\(\\bar{X}\\) is the sample mean,\n\\(n\\) is the sample size, and \\(w_i\\) is the weight assigned to the \\(i^{th}\\) observation.\nThe weights are computed iteratively using the following update rule:\n\\[\\begin{equation}\nw_i^{(k+1)} = \\frac{1}{\\sqrt{(X_i - \\bar{X})^T \\hat{\\Sigma}_{ARW}^{(k)-1} (X_i - \\bar{X})}},\n\\end{equation}\\]\nwhere \\(\\hat{\\Sigma}_{ARW}^{(k)}\\) is the estimate of the covariance matrix at iteration \\(k\\). The iterations continue until the weights converge to a stable value.\nThe main advantage of this method is that it can produce estimates of the covariance matrix that are robust to model misspecification, such as when the asset returns do not follow a multivariate normal distribution.\nSome limitations to using the Adaptive Re-weighted estimator include: the accuracy of the estimated covariance matrix depends on the choice of the re-weighting function. The Adaptive Re-weighted estimator may also produce unreliable estimates of the covariance matrix when the dataset contains highly correlated assets."
  },
  {
    "objectID": "posts/cov-mat-estimation-tangency-ports/index.html#benchmarks",
    "href": "posts/cov-mat-estimation-tangency-ports/index.html#benchmarks",
    "title": "A comparison of covariance matrix estimation methods for tangency portfolios.",
    "section": "Benchmarks",
    "text": "Benchmarks\nThe portfolio constructed using the sample covariance matrix is used as the benchmark against the other estimators."
  },
  {
    "objectID": "posts/high-dimensional-data/index.html",
    "href": "posts/high-dimensional-data/index.html",
    "title": "Learning representations in high-dimensional data",
    "section": "",
    "text": "High-dimensional data is the type of data1 which is characterized by the presence of many variables2. Due to the growing nature of variables of interest and data collection over the past years in diverse domains such as health-care/medicine, marketing, finance etc., there is an increasing need for techniques which are able to thrive in situations where the number of variables is higher, and at times even more than the number of data points available to train the model.31Â Also known as wide data2Â potentially where the number of variables(p) is greater than the number of observations in the sample(n) i.e.Â \\(p &gt; n\\).3Â For most Machine Learning algorithms, data at hand is usually of the form \\(n &gt;&gt; p\\), i.e.Â the number of data points(n) used in training the model is far higher than the number of predictors(p) in the data. However for high dimensional Machine Learning, the number of predictors(p) is usually very large, and at times more than the sample size(n), which poses a problem for most Machine Learning models.\nExamples of problems common in high dimensional learning include the following:\nPredicting consumer behavior and patterns in online-shopping stores, where the variables of interest, could be all search terms entered by the consumer, the click history, previous purchases and declines, demographic characteristics, and search account history. In such a problem, while the number of predictors for online behavior are many, we typically only have a few customer information.\nSignal generation, and price prediction in finance. In this domain, the variables of interest are usually: technical indicators of the price series such as the moving averages, volatility, etc, the fundamental indicators such as market capitalization and several accounting ratios, analyst ratings, social media sentiment etc. In this domain too, the number of historical data points used to train models is often limited4, however the number of predictors keeps growing.4Â at least not for high-frequency trading domain\nIn medicine, a problem of interest is to predict whether given tumors are benign or malignant, where variables would include a number of characteristics of cells e.g.Â perimeter, concavity, area, smoothness etc and other variables about the patient such as patientâs demographic characteristics, lifestyle characteristics etc. The characteristics could be so many, yet the number of patients, for which we have data could be few due to patients leaving studies/treatment.\n\nThe challenges associated with learning in high dimensions, require specialized techniques suited to such data since common statistical learning methods such as least squares fail in such dimensions. Potential dangers encountered when working with high-dimensional data include:\n\nMulti-collinearity: In the presence of a high number of predictors, the possibility of more than one pair of predictors being highly correlated increases, and this poses a challenge termed multi-collinearity in the data.5 Several machine learning models become unstable in the presence of multi-collinearity such as Neural networks, support vector machines etc, while some of them may break down completely such as multiple linear regression. Multi-collinearity introduces redundancy in model fitting, since two or more predictors attempt to explain the same variability in the response.\nFalse positive discoveries: In high dimension data, the probability of finding one or more predictors which are significantly related to the response due to random chance and not due to a true relationship increases, which leads to the problem of false discoveries. Such false positive findings often decrease a modelâs performance and hurt model interpretability.\n\n5Â Multi-collinearity refers to situations in which there are several predictors which are significantly correlated\n\n\n\n\n\nAn example of a dataset with two predictors, and two observations. (n = p = 2)\n\n\n\n\n\n\n\n\n\nAn example of a dataset with (n = p = 9) using polynomial regression (degree 9)\n\n\n\n\nOver-fitting: In high dimensional data, where \\(n = p\\) or \\(n &gt; p\\), then over-fitting is likely to occur. In this scenario, the models fitted have n degrees of freedom. This is illustrated in the following example: Suppose we have a sample of 2 data points, and one variable of interest(together with an intercept) i.e.Â \\(n = p\\), then fitting a linear regression model results in a perfect fit (all residuals become 0), however such a model may fail to generalize to previously unseen data(test data). This shows that in high dimensional learning, it is possible for models to perfectly fit the training data, and perform poorly in previously unseen data. In such cases, the training error is a poor approximation of test error rate.\nCommon performance metrics for models also fail in the high dimensional case, such as the \\(R^2, Adjusted-R^2\\) etc. This is because, for metrics such as \\(R^2\\), increasing the number of variables (p) in the model, almost always increases the \\(R^2\\) even when the variables have no significant relation to the response6. Consequently, possible collinearity among the predictors causes the tests of significance in models to be biased.\n\n6Â An example of an illustration showing what happens to a model when more variables which have no significant relationships to the response are added to the model. It is evident how adjusted R-squared almost always increases as the number of predictors increases, the training error always decreases as more predictors are added to a model due to possible over-fitting, but the test error increases, since the increased number of predictors add no predictive power to the model."
  },
  {
    "objectID": "posts/high-dimensional-data/index.html#ridge-regression",
    "href": "posts/high-dimensional-data/index.html#ridge-regression",
    "title": "Learning representations in high-dimensional data",
    "section": "Ridge regression",
    "text": "Ridge regression\nThis is a shrinkage based method for regression (suitable for \\(p &gt; n\\) data), which aims to supplement the Ordinary Least Squares method, especially in the context of high multi-collinearity.\nRecall, that for the orindary least squares model of the form:\n\\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_p x_p + \\epsilon_t\\]\nThe error function is of the form:\n\\[Q = \\sum{(y_i - \\hat{y_i})^2}\\] \\[where: \\hat{y_i} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_p x_p\\]\nIn slving for the coefficients of regression, we obtain the following closed-form solution:\n\\[\\beta = (X^T X)^{-1} (X^T Y)\\]\nHowever, in the prescence of many predictors, there is the ever-present risk of multi-collinearity, and thus the \\((X^T X)\\) matrix will not be of full rank, and hence not invertible. This in turn makes the coefficients of the regression model to grow large and unstable.\nA work around is to change the error function of the regression model to be:\n\\[Q_{L2} = \\sum{(y_i - \\hat{y_i})^2} + \\lambda_r \\sum{\\beta^2_j}\\]\nThis choice of error function, has the advantage that the error function remains a quadratic function of the regression coefficients, and its exact closed-form solution can be obtained by equating the gradient of the error function to \\(0\\), and solving for \\(\\beta\\) to obtain:\n\\[\\beta = (X^T X + \\lambda I)^{-1} (X^T Y)\\]\nThe \\(\\lambda\\) is called a penalty term or regularization coefficient, and this technique is called ridge regression. The penalty term must increase when the coefficients grow large, in order to enforce minimization. In result, the penalty causes the regression coefficients to become smaller and shrink towards 0, this makes the model much interpretable.\nThis particular choice of regularizer is known as weight decay in machine learning, or parameter shrinkage in statistics, since it has the tendancy to shrink parameter values towards 0"
  },
  {
    "objectID": "posts/high-dimensional-data/index.html#lasso-regression",
    "href": "posts/high-dimensional-data/index.html#lasso-regression",
    "title": "Learning representations in high-dimensional data",
    "section": "LASSO regression",
    "text": "LASSO regression\nA different choice of the regularizer could be obtained using the following error function:\n\\[Q_{L1} = \\sum{(y_i - \\hat{y_i})^2} + \\lambda_L \\sum{|\\beta_j|}\\]\nThis method is called: Least absolute shrinkage and selection operator: (LASSO). In modifying the error function to include the regularizer, lasso regression forces some regression coefficients to be 0, and in doing so, it practically selects model terms to an optimal number of predictors. This makes it a feature selection model.\nThe advantage of the LASSO regression over ridge regression is that: although ridge regression shrinks parameter estimates towards 0, it does not lead to any parameter estimates being 0, hence for the ridge regression, all (p) predictors are included in the model (which might hurt model interpretability). However, for the LASSO regression, the nature of its regularizer ensures that some parameter estimates are set to 0, hence effectively eliminating them from the model. Hence the LASSO regression has he advantage of producing simpler interpretable models than ridge regression. It should be noted however that this does not hurt the predictive ability of the ridge regression model."
  },
  {
    "objectID": "posts/high-dimensional-data/index.html#elastic-net-regression-combining-ridge-and-lasso-regression",
    "href": "posts/high-dimensional-data/index.html#elastic-net-regression-combining-ridge-and-lasso-regression",
    "title": "Learning representations in high-dimensional data",
    "section": "Elastic-Net Regression (Combining Ridge and LASSO regression)",
    "text": "Elastic-Net Regression (Combining Ridge and LASSO regression)\nSince ridge regression has the advantage of combating multi-collinearity, and the LASSO regression has the advantage of being a feature/variable selection model, the two models can be combined, in order to deal with both multi-collinearity, and feature selection at once.\nThe form of the error function of model is shown below:\n\\[Q = \\sum{(y_i - \\hat{y_i})^2} + \\lambda [ (1 - \\alpha)\\sum{\\beta^2_j} + \\alpha \\sum{| \\beta_j |} ]\\]\nHere, \\(\\lambda = \\lambda_r + \\lambda_L\\) , and the proportion of \\(\\lambda\\) associated with the lasso is denoted \\(\\alpha\\). Thus, selecting \\(\\alpha = 1\\) would be a full lasso penalty model, selecting \\(\\alpha = 0\\) would be a full ridge regression model, whereas \\(\\alpha = 0.5\\) is an even mix of a ridge and lasso model."
  },
  {
    "objectID": "posts/high-dimensional-data/index.html#search-for-optimal-lambda",
    "href": "posts/high-dimensional-data/index.html#search-for-optimal-lambda",
    "title": "Learning representations in high-dimensional data",
    "section": "Search for optimal \\(\\lambda\\)",
    "text": "Search for optimal \\(\\lambda\\)\nThe optimal value of \\(\\lambda\\) for the ridge and LASSO regression model is found by means of cross-validation, where several choices of \\(\\lambda\\) are used on the training set, and the performance of the models are evaluated on a validation set,so that the value of \\(\\lambda\\) which yields the least training error, is preferred. For Elastic-Net regression, a common method of selecting the best regularization coefficient, is to construct a grid of \\(\\alpha\\) values, and for each value of \\(\\alpha\\), the best regularization coefficient \\(\\lambda\\) is found. The fitted models are then compared based on validation error."
  },
  {
    "objectID": "posts/high-dimensional-data/index.html#principal-components-analysis",
    "href": "posts/high-dimensional-data/index.html#principal-components-analysis",
    "title": "Learning representations in high-dimensional data",
    "section": "Principal Components Analysis",
    "text": "Principal Components Analysis\n\n\nRecall that, for a data matrix A, and an identity matrix I, the eigen values are \\(\\lambda\\) such that: \\[|A - \\lambda I| = 0\\] The corresponding eigen vector \\(\\hat{v}\\), of an eigen value \\(\\lambda\\) satisfies the equation: \\[(A - \\lambda I) \\hat{v} = 0\\]\nPrincipal Components Analysis (PCA) is the most popular dimensionality reduction method. The aim of PCA is to find a subset of predictors, which is esentially a linear combination of the original predictor space, such that the combinations explain maximal variability of the original predictor space.\nIn PCA, the new features formed7, are usually orthogonal to each other8. This makes it a very useful tool in dealing with multi-collinearity.7Â Often called scores or components8Â Implying theyâre uncorrelated, and thus there is minimal overlap in the information provided by each score\nWe consider an \\(n*p\\) centered data matrix \\(X\\), where n is the number of observations, and p is the number of predictors. We then create a \\(p*p\\) matrix, whose columns are eigen vectors of \\((X^T X)\\).\nThe matrix \\(W\\) is the matrix of unit eigen vectors. In constructing \\(W\\), we usually ensure that eigen vectors are ranked by the highest eigen value i.e.Â components with the highest explanatory power come first. It follows that \\(W\\) is orthogonal, i.e.Â \\(W^T = W^{-1}\\)\nThe principal components decomposition \\(P\\) of \\(X\\) is then defined as: \\(P= XW\\)\nA popular application of principal components analysis is principal components regression, where the predictor matrix is first reduced into a matrix of scores using PCA, and this matrix of scores is then fed into regression."
  },
  {
    "objectID": "posts/high-dimensional-data/index.html#kernel-principal-components-analysis",
    "href": "posts/high-dimensional-data/index.html#kernel-principal-components-analysis",
    "title": "Learning representations in high-dimensional data",
    "section": "Kernel Principal Components Analysis",
    "text": "Kernel Principal Components Analysis\nRecall that PCA is useful in forming component by extracting linear combinations of predictors from the original predictor space, hence it is useful only when there are linear patterns in the predictor space.\nBut supposing that, the functional form of the data at hand is given by the following equation below:\n\\[y = x_1 + x_2 + x_1^2 + x_2^2 + \\epsilon_t\\]\nThen, using PCA will only construct linear combinations of \\(x_1\\) and \\(x_2\\), thus missing out the important quadratic relationships in the data.\nThus in the presence of possible non-linear relationships in the data, Kernel-PCA is better suited.\nK-PCA extends PCA using kernel methods, so that for linear combinations of variables, K-PCA captures this using the linear kernel:\n\\[k(x_1, x_2) = x_1^Tx_2\\]\nAlthough the linear kernel could be substituted using any other kernel of choice, such as the polynomial kernel:\n\\[k(x_1, x_2) = &lt;x_1, x_2&gt;^d\\] so that for quadratic relationships, we set \\(d = 2\\):\n\\[k(x_1, x_2) = &lt;x_1, x_2&gt;^2 = (x_{11}x_{12} + ... + x_{n1}x_{n2})^2 \\]"
  },
  {
    "objectID": "posts/high-dimensional-data/index.html#independent-components-analysis",
    "href": "posts/high-dimensional-data/index.html#independent-components-analysis",
    "title": "Learning representations in high-dimensional data",
    "section": "Independent Components Analysis",
    "text": "Independent Components Analysis\nRecall, PCA forms scores using linear combinations of the original predictor space such that the new scores formed are orthogonal with each other, and thus uncorrelated, however this does not mean that the scores are statistically independent of each other.99Â This is because in certain cases, the correlation could be 0, however the covariance could be indicating otherwise, except in cases where data comes from the gaussian distribution, where un-correlation implies independence.\nICA bears some similarity with PCA10, however in creating the scores, it does so in a way that the scores are statistically independent of each other. Generally, ICA tends to model a broader set of trends than PCA, which is only concerned with orthogonality.10Â It should however be noted that scores generated by ICA are different from PCA scores\nGiven a random observed vector \\(X\\),whose elements are mixtures of independent elements of a random vector \\(S\\) given by:1111Â Both \\(X\\) and \\(S\\) are vectors of length \\(m\\)\n\\[X = AS\\]\nWhere \\(A\\) denotes a mixing matrix of size \\(m*m\\), the goal of ICA is to find the un-mixing matrix \\(W\\)12, that will give the best approximation of \\(S\\)12Â An inverse of the mixing matrix \\(A\\)\n\\[WX \\approx S\\]\nICA makes the following assumptions about data:\n\nStatistical independence in the source signal\nMixing matrix must be a square matrix of full rank.\nThe only source of randomness is the vector \\(S\\).\nThe data at hand is centered and whitened.13\nThe source signals must not have a gaussian distribution except for only one source signal.\n\n13Â Centered data is data which has been demeaned, and whitening could be achieved by first running PCA on the original data and using the whole set of components as input data to ICAICA constructs scores based on two methods:\n\nMinimization of mutual Information\n\nFor a pair of random variables \\(X, Y\\), the mutual information is defined as follows:\n\\[I(X;Y) = H(X) - H(X|Y)\\] Where:\n\\(H(X)\\): is the entropy of \\(X\\).\n\\[H(X) = - \\sum_x{P(x)\\log{P(x)}}\\]\n\\(H(X|Y)\\): is the conditional entropy.1414Â The entropy of \\(X\\) conditional on \\(Y\\) taking a certain value \\(y\\)\n\\[H(X|Y) = H(X, Y) - H(Y)\\]\nwhere:\n\\(H(X, Y)\\): is the joint entropy given by:\n\\[H(X, Y) = - \\sum_{x, y}{P(x, y) \\log{P(x, y)}}\\]\nFrom the above equations, entropy can be seen as a measure of uncertainty of information in a random variable, so that the lower the value of entropy, the more information we have about the random variable of interest. Therefore by seeking for a method of maximizing mutual information, we would be seeking for components which are maximally independent.\n\nMaximization of non-gaussianity.\n\nThis is a second method of constructing independent components. Since in the assumptions underlying ICA, is the assumption of non-gaussianity of the source signals, then, one way of extracting components is to maximize non-gaussianity of the components.15.15Â Forcing the components to be as far as possible from the gaussian distribution\nAn example of a non-gaussianity measure is the Negentropy, given by:\n\\[N(X) = H(X^N) - H(X)\\]\nWhere:\n\\(X\\): is a random non-gaussian vector.\n\\(X^N\\): is a gaussian random vector with same covariance matrix as \\(X\\).\n\\(H(.)\\): is the entropy.\nSice the gaussian distribution has the highest entropy for any given covariance matrix,then the negentropy: \\(N(X)\\) is a strictly positive measure of non-gaussianity."
  },
  {
    "objectID": "posts/high-dimensional-data/index.html#partial-least-squares",
    "href": "posts/high-dimensional-data/index.html#partial-least-squares",
    "title": "Learning representations in high-dimensional data",
    "section": "Partial Least Squares",
    "text": "Partial Least Squares\nPartial Least Squares (PLS) is a supervised dimensionality reduction method, in that the response variable is used in guiding the dimensionality reduction process unlike in the context of PCA. Hence in constructing the components, PLS does so in a way that the components not only summarize maximal variability in the predictor space, but also are related to the response significantly.\nGiven \\(p\\) predictors: \\(X_1, X_2, ... , X_p\\), and the response variable \\(Y\\), we construct linear combinations of our original predictors: \\(Z_1, ..., Z_m, \\hspace{2 mm} m &lt; p\\), components:\n\\[Z_m = \\sum_{j=1}^p {\\phi_{jm} X_j}\\]\nWhere: \\(\\phi_{jm}\\): are some constants. In computing the first PLS direction \\(Z_1\\), PLS sets each \\(\\phi_{j1}\\) equal to the coefficient from a simple linear regression of \\(Y\\) onto \\(X_j\\) 16, hence it is evident that PLS places larger weight on variables which are highly correlated to the response variable. The second PLS direction is first computed by taking the residuals after regression each variable on \\(Z_1\\)17. The second PLS direction: \\(Z_2\\) is computed using the orthogonalized data in the same fashion as \\(Z_1\\), and this procedure is repeated to obtain the \\(m\\) PLS components.16Â It can be shown that this coefficient is proportional to the correlation between \\(X_j\\) and \\(Y\\)17Â The residuals are interpreted as: amunt of information that has not been accounted for by the first PLS direction"
  },
  {
    "objectID": "posts/high-dimensional-data/index.html#ridge-regression-1",
    "href": "posts/high-dimensional-data/index.html#ridge-regression-1",
    "title": "Learning representations in high-dimensional data",
    "section": "Ridge Regression",
    "text": "Ridge Regression\n\n\n\n\n\n\nCross validation statistics for estimating the regularization parameter of ridge regression, and their error bars. The dotted line represent estimate of lambda which is within its one standard error\n\n\n\nWe proceed to fit ridge regression on the data, and select the regularization parameter using cross-validation21. The cross validation statistics are shown below:21Â The best estimate for \\(\\lambda\\) using cross validation was found to be: 12.75054\n\n\n\n\n\nThe model fitted using the regularization parameter obtained by cross validation (Ridge CV), has roughly 70% of the model coefficients shrunken to be close to 0, showing how effective ridge regression is in producing interpretable models."
  },
  {
    "objectID": "posts/high-dimensional-data/index.html#lasso-regression-1",
    "href": "posts/high-dimensional-data/index.html#lasso-regression-1",
    "title": "Learning representations in high-dimensional data",
    "section": "LASSO Regression",
    "text": "LASSO Regression\nWe proceed to fit LASSO regression on the data, and select the regularization parameter using cross-validation22. The cross validation statistics are shown below:22Â The best estimate for \\(\\lambda\\) using cross validation was found to be: 0.1893415.\n\n\n\n\n\n\nCross validation statistics for estimating the regularization parameter of LASSO regression and their error bars. The dotted line represent estimate of lambda which is within its one standard error\n\n\n\n\n\n\n\n\nThe model fitted using the regularization parameter obtained from cross validation as shown above has forced majority of the model coefficients to be 0, thereby removing the variables from the model. The LASSO regression technique is therefore important in variable selection, since by setting some model coefficients to 0, it effectively removes them from the model, leaving us with a much smaller and interpretable model."
  },
  {
    "objectID": "posts/high-dimensional-data/index.html#elastic-net-regression",
    "href": "posts/high-dimensional-data/index.html#elastic-net-regression",
    "title": "Learning representations in high-dimensional data",
    "section": "Elastic Net Regression",
    "text": "Elastic Net Regression\nIn this section, we fit an Elastic Net model, which is a mixture of both ridge and LASSO regression. We select the mixing-weight based on two methods:\n\nWe compute the model regularization parameter \\(\\lambda\\), as a sum of the cross validation value of \\(\\lambda\\) computed in ridge regression, and that computed from LASSO regression. i.e.\n\n\\[\\lambda = \\lambda_R + \\lambda_L\\] We then run a cross validation using this fixed \\(\\lambda\\) on several values of \\(\\alpha\\), and obtain the statistics as shown in the following chart:\n\n\n\n\n\nUsing the regularization parameter obtained from cross validation\n\n\n\n\n\nIn this second method, we construct a grid of \\(\\alpha\\) values which are equally spaced on the range \\([0, 1]\\), and for each \\(\\alpha_i\\), we perform cross validation on the training set to obtain the most suitable value of the regularization parameter \\(\\lambda_i\\).23 The result is shown below:24\n\n23Â This is the most suitable technique to use in Elastic-Net regression.24Â Cross validation is performed to determine the best value for the regularization coefficient for every value of alpha chosen. The value of alpha = 0.386387387, and the corresponding lambda = 0.1106008, gave the lowest training error(0.4), as well as the highest deviance(37%), using only 19 non-zero model coefficients.\n\n\n\n\nFrom the above chart, it shows that, as the value of \\(\\alpha\\) increases, then the regularization parameter \\(\\lambda\\) reduces, which shows that for this model, a very small proportion of \\(\\lambda\\) was attributed to the LASSO penalty. The deviance resulting from this is quite low (less than 30%). The training error, as well as the deviance are suitable for small values of alpha chosen. For the Elastic Net regression, we will proceed with this \\(2^{nd}\\) model hyper-parameters, since it gives a lower training error, for few variables, as compared to the rest."
  },
  {
    "objectID": "posts/high-dimensional-data/index.html#principal-components-regression",
    "href": "posts/high-dimensional-data/index.html#principal-components-regression",
    "title": "Learning representations in high-dimensional data",
    "section": "Principal Components Regression",
    "text": "Principal Components Regression\n\n\n\n\n\n\nIt is evident that PCA constructs components in a way that they are orthogonal to each other and hence not correlated. This helps in dealing with the multi-collinearity present in the data.\n\n\n\nIn this section, Principal components analysis model is fitted using only 56 principal components and the results of the Principal Components Regression are displayed.\n\n\n\n\n\nFrom the scree-plot above, it is evident that the first two components account for maximal variability in the predictor matrix. In choosing the suitable number of components to run regression with, we examine the plot of cross-validation error below:\n\n\n\n\n\nFrom the validation plot using RMSE as the error metric, the model with the lowest cross validation error is the 2-components model, which we will proceed with."
  },
  {
    "objectID": "posts/high-dimensional-data/index.html#kernel-principal-components-regression",
    "href": "posts/high-dimensional-data/index.html#kernel-principal-components-regression",
    "title": "Learning representations in high-dimensional data",
    "section": "Kernel Principal Components Regression",
    "text": "Kernel Principal Components Regression\n\n\noper 1 step center [training] \noper 2 step scale [training] \nThe retained training set is ~ 0.05 Mb  in memory.\n\n\noper 1 step center [training] \noper 2 step scale [training] \noper 3 step kpca [training] \nThe retained training set is ~ 0.02 Mb  in memory.\n\n\n\n\n\n\n\n\nIt is evident that k-PCA constructs components in a way that they are orthogonal to each other and hence not correlated. This helps in dealing with the multi-collinearity present in the data.\n\n\n\nIn this section, the Kernel-PCA is first performed on the predictor matrix, and then the most optimal subset of the resulting components constructed is used to fit a linear regression model on our training dataset. For the Kernel-PCA, we chose a radial basis kernel, where the hyper-parameter \\(\\sigma\\) was chosen automatically based on our data.\n\n\nThe estimated value of \\(\\sigma\\), is based upon the 10%, and 90% quantile of \\(||x - x^`||^2\\), where we chose \\(\\sigma\\) as the median value of: 0.008732801.\nThe charts below show the percentage variability in the original predictor matrix explained by the resulting kernel principal components:\n\n\n\n\n\nFrom the scree plot on chart 1, it is evident that the first 4 kernel principal components explain maximal variability in the original predictor matrix. The cross validation training error increases as more components are added into the model. In selecting the optimal number of principal components to include in the model, we select 10 components, since this gives the highest amount of variability explained in the response variable."
  },
  {
    "objectID": "posts/high-dimensional-data/index.html#partial-least-squares-1",
    "href": "posts/high-dimensional-data/index.html#partial-least-squares-1",
    "title": "Learning representations in high-dimensional data",
    "section": "Partial Least Squares",
    "text": "Partial Least Squares\n\n\n\n\n\n\nIt is evident that PLS also constructs components in a way that they are orthogonal to each other and hence not correlated. This helps in dealing with the multi-collinearity present in the data.\n\n\n\nThis section covers the analysis section for the partial least squares model. The PLS model is fitted using cross-validation, and the data is centered and scaled before the model fitting process.\nThe scree-plot for the PLS model is shown below\n\n\n\n\n\nThe above chart shows that the first 4 components explain majority of the variability in the original predictor matrix (roughly 70%). The Training error from cross validation is shown below:\n\n\n\n\n\nFrom this chart, we select only the first component, to include in our final PLS model, since it gives the lowest cross validation error. A comparison of the variability in the response explained by the PCR and PLS model is shown below, in order to capture the difference between PLS and PCR.\nTo examine the difference between PLS and PCA in explaining the response variable, we examine the (%) variance explained in the response by each component as shown below:\n\n\n\n\n\nFrom the chart above, it is evident that for any number of principal components, the PLS explains the highest variability in the response variable, since it is a supervised dimensionality reduction technique, where the response variable guides the reduction process, as compared to the PLS which is an unsupervised technique."
  },
  {
    "objectID": "posts/high-dimensional-data/index.html#independent-components-analysis-1",
    "href": "posts/high-dimensional-data/index.html#independent-components-analysis-1",
    "title": "Learning representations in high-dimensional data",
    "section": "Independent Components Analysis",
    "text": "Independent Components Analysis\n\n\noper 1 step ica [training] \nThe retained training set is ~ 0.02 Mb  in memory.\n\n\n\n\n\n\n\n\nIt is evident that ICA also constructs components in a way that they are STATISTICALLY INDEPENDENT to each other and hence not correlated. This helps in dealing with the multi-collinearity present in the data.\n\n\n\nThis section gives a summary of the analysis performed using Independents Components Analysis. For the ICA, only 30 independent components are constructed. Results from the cross validation analysis performed on ICA features is shown below:\n\n\n\n\n\nBased on the cross validation plots, we proceed with an regression model fitted with only the first ICA components."
  },
  {
    "objectID": "posts/high-dimensional-data/index.html#comparison-of-models",
    "href": "posts/high-dimensional-data/index.html#comparison-of-models",
    "title": "Learning representations in high-dimensional data",
    "section": "Comparison of models",
    "text": "Comparison of models\nIn this section, we compare all models fitted on the testing data. We use the mean squared error to gauge the best models.\n\n\n\n\n\nmodel\nTraining.error\nTesting.error\n\n\n\n\nRidge(CV)\n0.5524857\n0.4890077\n\n\nLASSO(CV)\n0.5690525\n0.4767853\n\n\nElastic-Net\n0.4072456\n0.4338640\n\n\nPCR\n0.5830379\n0.5439550\n\n\nPLS\n0.5536653\n0.5070605\n\n\nk-PCA\n0.3623230\n0.4342610\n\n\nICA\n0.6424859\n0.4966551\n\n\n\n\n\nThe charts on performance are shown below:\n\n\n\n\n\nFrom the above charts and statistics, it is evident that the Kernel PCA, and Elastic-Net regression model emerge the best, and their performance in both the training set and testing set is consistent. The PCR, PLS and ICA model offer a poor fit to the data in both two sets of data. The Ridge and LASSO regression models have more less the same performance in both sets of data. The performance of Kernel PCA indicates that there exists some non-linear dependencies on the data - which Kernel-PCA is good at uncovering as compared to PCA."
  },
  {
    "objectID": "posts/high-dimensional-data/index.html#ridge-regression-2",
    "href": "posts/high-dimensional-data/index.html#ridge-regression-2",
    "title": "Learning representations in high-dimensional data",
    "section": "Ridge Regression",
    "text": "Ridge Regression\n\n\n\n\n\n\nCross validation statistics for estimating the regularization parameter of ridge regression, and their error bars. The dotted line represent estimate of lambda which is within its one standard error\n\n\n\nIn this section, we analyze a ridge regression model for classification A suitable regularization parameter was obtained using cross-validation25. The model fit statistics are shown below:25Â The best estimate for \\(\\lambda\\) using cross validation was found to be: 2.610173\n\n\n\n\n\nIn the chart above, it is evident that about 60% of the model coefficients have shrunk to be close to 0, showing how effective ridge regression is in producing interpretable models. The performance of the ridge regression model on the training dataset is shown in the Receiver Operating Characteristic Curve, with an AUC of: 0.85435."
  },
  {
    "objectID": "posts/high-dimensional-data/index.html#lasso-regression-2",
    "href": "posts/high-dimensional-data/index.html#lasso-regression-2",
    "title": "Learning representations in high-dimensional data",
    "section": "LASSO Regression",
    "text": "LASSO Regression\n\n\n\n\n\n\nCross validation statistics for estimating the regularization parameter of LASSO regression, and their error bars. The dotted line represent estimate of lambda which is within its one standard error\n\n\n\nIn this section, we analyze the LASSO regression model fitted. A suitable regularization parameter was obtained using cross-validation26. The model fit is displayed below:26Â The best estimate for \\(\\lambda\\) using cross validation was found to be: 0.07095985\n\n\n\n\n\nIn the chart above, it is evident that about 87% of the model coefficients have set to 0, showing how effective LASSO regression is in feature selection. The performance of the LASSO regression model on the training dataset is shown in the Receiver Operating Characteristic Curve, with an AUC of: 0.8592."
  },
  {
    "objectID": "posts/high-dimensional-data/index.html#elastic-net-regression-1",
    "href": "posts/high-dimensional-data/index.html#elastic-net-regression-1",
    "title": "Learning representations in high-dimensional data",
    "section": "Elastic-net Regression",
    "text": "Elastic-net Regression\n\n\n\n\n\n\nThe Receiver Operating Characteristic curve for the ElasticNet model using an alpha = 0.062063062, and lambda = 0.62452991. The Area Under Curve (AUC) is: 0.8641\n\n\n\nIn this section, the fit of the mixture of LASSO and Ridge regression on the data is shown. Suitable values for the mixing weight \\(\\alpha\\), and the redularization parameter, \\(\\lambda\\) are found using cross validation, where for a fixed value of \\(\\alpha\\), the best \\(\\lambda\\) is searched for, and several accuracy metrics are computed.\n\n\n\n\n\nFrom the charts above, it is evident that as the \\(\\alpha\\) increases, then the model tends to be more sparse, the deviance explained decreases while the training error rate decreases. The best combination of the \\(\\alpha\\), and \\(\\lambda\\) parameter are chosen to minimize the cross validation error."
  },
  {
    "objectID": "posts/high-dimensional-data/index.html#principal-components-analysis-1",
    "href": "posts/high-dimensional-data/index.html#principal-components-analysis-1",
    "title": "Learning representations in high-dimensional data",
    "section": "Principal Components Analysis",
    "text": "Principal Components Analysis\n\n\noper 1 step center [training] \noper 2 step scale [training] \noper 3 step pca [training] \nThe retained training set is ~ 0.02 Mb  in memory.\n\n\nIn this section, we ran the principal components analysis model using 30 principal components and the results of cross validation on the principal components are displayed.\n\n\n\n\n\n\n\n\n\n\n\nPCA is applied to the dataset, and the first two principal components are plotted, and coloured by the Direction variable. For the Receiver Operating Characteristic, the AUC is: 0.8084\n\n\n\nFrom the above charts, the first three principal components explain maximal variability in the predictor space. Cross validation on the training set indicates that, the first three principal components give the best model in terms of Overall accuracy and Kappa. Hence for the purpose of model fitting, we will only use three principal components."
  },
  {
    "objectID": "posts/high-dimensional-data/index.html#kernel-principal-components-analysis-1",
    "href": "posts/high-dimensional-data/index.html#kernel-principal-components-analysis-1",
    "title": "Learning representations in high-dimensional data",
    "section": "Kernel Principal Components Analysis",
    "text": "Kernel Principal Components Analysis\n\n\noper 1 step center [training] \noper 2 step scale [training] \nThe retained training set is ~ 0.05 Mb  in memory.\n\n\noper 1 step center [training] \noper 2 step scale [training] \noper 3 step kpca [training] \nThe retained training set is ~ 0.02 Mb  in memory.\n\n\nIn this section, the Kernel principal components analysis model is fitted using 30 components and the cross validation results are shown below:\n\n\n\n\n\n\n\n\n\n\n\nKernel PCA is applied to the dataset, and the first two principal components are plotted, and coloured by the Direction variable. For the Receiver Operating Characteristic, the AUC is: 0.7908\n\n\n\nFrom the above charts, the first four components explain maximal variability in the predictor space. Cross validation on the training set indicate that, only the first two or five components give the best model in terms of Overall accuracy and Kappa. Hence for the purpose of model fitting, we will only use two components27.27Â This is because for the 5-component and 2-component model,there is no big difference, hence we fit a model using 2 components only to ensure parsimonity"
  },
  {
    "objectID": "posts/high-dimensional-data/index.html#independent-components-analysis-2",
    "href": "posts/high-dimensional-data/index.html#independent-components-analysis-2",
    "title": "Learning representations in high-dimensional data",
    "section": "Independent Components Analysis",
    "text": "Independent Components Analysis\n\n\noper 1 step center [training] \noper 2 step scale [training] \noper 3 step ica [training] \nThe retained training set is ~ 0.02 Mb  in memory.\n\n\nThis section covers the application of Independent Components Analysis to the classification dataset. We restrict the number of independent Components to 30 components.\n\n\n\n\n\n\n\n\n\n\n\nICA is applied to the dataset, and the first two Independent components are plotted, and coloured by the Direction variable. For the Receiver Operating Characteristic, the AUC is: 0.7898\n\n\n\nFrom the charts above, the highest overall accuracy and Kappa are obtained when using 15 independent components, although as compared to the cross validation performance of the other models above, the ICA under-performs all models. We proceed to fit a GLM using the first 15 components."
  },
  {
    "objectID": "posts/high-dimensional-data/index.html#comparison-of-models-1",
    "href": "posts/high-dimensional-data/index.html#comparison-of-models-1",
    "title": "Learning representations in high-dimensional data",
    "section": "Comparison of Models",
    "text": "Comparison of Models\nIn this section, we compare all fitted models, on the test dataset. For comparison, we use the Overall Accuracy, although other metrics of classification models are quoted. For the cutoff probability, we select the cutoff which gave the highest Youden statistic on the training data.\n\n\n\n\n\n\nThe Receiver Operating characteristics for the models fitted on the training dataset\n\n\n\n\n\n\n\n\nThe training set performance metrics are displayed below:\n\n\n\n\n\n\n\nModel\n\n\nAccuracy\n\n\nKappa\n\n\nSensitivity\n\n\nSpecificity\n\n\nPPV\n\n\nNPV\n\n\nF1\n\n\n\n\n\n\nRidge\n\n\n0.812500\n\n\n0.6238981\n\n\n0.7741935\n\n\n0.8484848\n\n\n0.8275862\n\n\n0.8000000\n\n\n0.8000000\n\n\n\n\nLASSO\n\n\n0.812500\n\n\n0.6231600\n\n\n0.7419355\n\n\n0.8787879\n\n\n0.8518519\n\n\n0.7837838\n\n\n0.7931034\n\n\n\n\nElastic-Net\n\n\n0.812500\n\n\n0.6238981\n\n\n0.7741935\n\n\n0.8484848\n\n\n0.8275862\n\n\n0.8000000\n\n\n0.8000000\n\n\n\n\nPCA\n\n\n0.781250\n\n\n0.5620723\n\n\n0.7741935\n\n\n0.7878788\n\n\n0.7741935\n\n\n0.7878788\n\n\n0.7741935\n\n\n\n\nk-PCA\n\n\n0.718750\n\n\n0.4347399\n\n\n0.6451613\n\n\n0.7878788\n\n\n0.7407407\n\n\n0.7027027\n\n\n0.6896552\n\n\n\n\nICA\n\n\n0.765625\n\n\n0.5275591\n\n\n0.6451613\n\n\n0.8787879\n\n\n0.8333333\n\n\n0.7250000\n\n\n0.7272727\n\n\n\n\n\nPerformance of Models on the training set\n\n\n\nThe testing set performance metrics are displayed below:\n\n\n\n\n\n\n\nModel\n\n\nAccuracy\n\n\nKappa\n\n\nSensitivity\n\n\nSpecificity\n\n\nPPV\n\n\nNPV\n\n\nF1\n\n\n\n\n\n\nRidge\n\n\n0.6250\n\n\n0.2835821\n\n\n0.4444444\n\n\n0.8571429\n\n\n0.8000000\n\n\n0.5454545\n\n\n0.5714286\n\n\n\n\nLASSO\n\n\n0.5625\n\n\n0.1515152\n\n\n0.4444444\n\n\n0.7142857\n\n\n0.6666667\n\n\n0.5000000\n\n\n0.5333333\n\n\n\n\nElastic-Net\n\n\n0.5625\n\n\n0.1515152\n\n\n0.4444444\n\n\n0.7142857\n\n\n0.6666667\n\n\n0.5000000\n\n\n0.5333333\n\n\n\n\nPCA\n\n\n0.7500\n\n\n0.4920635\n\n\n0.7777778\n\n\n0.7142857\n\n\n0.7777778\n\n\n0.7142857\n\n\n0.7777778\n\n\n\n\nk-PCA\n\n\n0.8125\n\n\n0.6250000\n\n\n0.7777778\n\n\n0.8571429\n\n\n0.8750000\n\n\n0.7500000\n\n\n0.8235294\n\n\n\n\nICA\n\n\n0.6250\n\n\n0.2615385\n\n\n0.5555556\n\n\n0.7142857\n\n\n0.7142857\n\n\n0.5555556\n\n\n0.6250000\n\n\n\n\n\nPerformance of Models on the testing set\n\n\n\nFrom the statistics and charts above, it is evident that Kernel PCA outperforms all other models on the testing set, with an overall accuracy of 81.25%, with all other models having accuracy below 80%.\nThe good performance exhibited by Kernel PCA over PCA, shows that there were some non-linear relationships within the predictors."
  },
  {
    "objectID": "posts/DiD UHC Pilot/index.html",
    "href": "posts/DiD UHC Pilot/index.html",
    "title": "The impact of the pilot-UHC: a differences-in-difference approach",
    "section": "",
    "text": "The author is grateful to Angela Langat1 for her helpful comments in the analysis and writing of this article.1Â Link to her socials\n\nIntroduction\nTo mark Kenyaâs commitment to the SDG 3 goal on ensuring healthy lives and promoting well-being for all, the government of Kenya introduced the Universal Health Coverage (UHC) in December 20182. The aim of the UHC was to strengthen primary health care in Kenya by ensuring that citizens have access to a progressive health benefit and increase the availability and readiness of key health interventions.2Â Nzwili, Fredrick. 2018. âKenyan President Launches Benchmark Universal Health Coverage Pilot, To Become Nationwide In 18 Months.â Health Policy Watch (blog). 2018. https://www.healthpolicy- watch.org/kenyan-president-launches- benchmark-universal-health-coverage-pilot- to-become-nationwide-in-18-months/\nThe first phase of the UHC was conducted in a pilot project targeting 4 counties of Kenya: Isiolo, Kisumu, Machakos and Nyeri. The four counties were chosen on the basis that: they are characterized by high incidence of both communicable and non-communicable diseases especially diabetes and hypertension, high population density, high maternal mortality rates, and high incidence of road traffic injuries. Isiolo was chosen to experiment with how the UHC would fare in a majorly pastoral county.\n\n\n\n\n\n\nFigureÂ 1: Location of the counties selected for the pilkot UHC program\n\n\n\nThe UHC was funded by the national government and a directive was given to the county governments to abolish fees in the level 4 and 5 facilities3. The government would then work out a reimbursement plan using conditional grants with the county governments for the forgone fees. The pilot phase of the UHC was intended to run for a year. In this, the government intended to gain lessons and insights that would be useful in scaling up the UHC to the remaining 43 counties nation-wide. Later in February 2022, the president Uhuru Kenyatta rolled out the UHC across all counties after a successful pilot4.3Â ref4Â ref\n\n\nData\nIn this analysis, we set out to investigate the impact of the UHC pilot program on health accessibility in Kenya. Specifically: what was the impact of the pilot UHC program on health accessibility in maternal health indicators such as: ANC attendance and skilled delivery.\nThe data sources chosen for this study are: the Kenya DHS 2014 and Kenya DHS 2022 and the KNBS national statistical abstracts.\nThe two are chosen as:\n(i). They offer good data on several key maternal and child health indicators such as skilled birth attendance, antenatal care coverage, road accident data, HIV data etc.\n(ii). They have data available for the pre and post UHC period, as the 2014 DHS covers the period 2010-2014 (pre UHC) and the 2022 DHS covers the 2020-2022 (post UHC) period. This is useful in studying the impact of the health intervention (rolling out of the UHC). For the statistical abstracts, the data is available for yearly periods.\nThe assumptions we are making in this study are as follows:\n\nThe DHS surveys samples are large enough and nationally representative, and have sufficient coverage of every county. This ensures that the estimates we get from the analysis are robust and reliable.\nThat the UHC was rolled out perfectly (i.e.Â that it was actually utilized by the people in the pilot counties), and that peoiple from the non-pilot counties were not able to access the healthcare in the counties selected for the UHC.\n\n\n\nAnalysis\nA set of questions were asked to women aged 15-49 years, on the difficulties they encounter when seeking medical help for themselves when they fall sick. The difficulties asked included: money, distance to health facility, permission to seek care, fear of going alone to health facilities.\n\n\n\n\n\nFigureÂ 2: Response to questions on health accessibility to women aged 15-49 years. The pilot UHC counties are highlighted in labelled ocolors while the rest of the non-poilot counties are represented with the gray lines.\n\n\n\n\nThe proportion of women who complained about money being the difficulty to health care accessibility in Kisumu and Isiolo rose sharply from 2014 to 2022. Isiolo county rose from a 4% prevalence to a 30% prevalence, a 26% absolute increase. The counties of Nyeri and Machakos experienced drops, albeit not large (Nyeri: 1% absolute decrease, Machakos: 8% absolute decrease).\nThe proportion of women complaining about distance being a hindrance to health accessibility in Kenya is overall-y low, with the highest county prevalence in 2014 being below 15%, and the highest in 2022 being less than 10% indicating an improvement in health coverage. Among the pilot UHC counties, only Isiolo encountered an increase in the proportion of women complaining about distance being an issue. The rest of the three counties experienced declines in the indicator.\nAmong women who complained about distance and money jointly, Kisumu was the leading in terms of an increase in that indicator( rise from 33% to 50%). Only Machakos experienced a decline (of 1% absolute) between the two surveys. This is likely insignificant."
  },
  {
    "objectID": "posts/glms-scratch/index.html",
    "href": "posts/glms-scratch/index.html",
    "title": "Generalized Linear Models: from scratch",
    "section": "",
    "text": "Image taken from: Pietro Ravani, Patrick Parfrey, Sean Murphy, Veeresh Gadag, Brendan Barrett, Clinical research of kidney diseases IV: standard regression models, Nephrology Dialysis Transplantation, Volume 23, Issue 2, February 2008, Pages 475â482, https://doi.org/10.1093/ndt/gfm880\n\nIntroduction\nIf you are reading this, you are likely already acquainted with the simple linear regression model and its extension, the multiple linear regression model. If not, I recommend familiarizing yourself with these foundational concepts before proceeding.\n\n\n\n\n\n\nFigureÂ 1: Simple linear regression model fitted on the dataset: D: X ~ N(0, 1), Y ~ N(2 + 3*X, 1)\n\n\n\nGeneralized Linear Models (GLMs) represent a valuable extension of the linear model framework, particularly when the response variable of interest follows a non-Gaussian distribution. Recall that in Ordinary Least Squares (OLS) regression, a crucial assumption is that the modelâs error terms (and by extension, the response variable) are normally distributed. This assumption is instrumental when conducting statistical inference on the estimated coefficients, such as constructing confidence intervals and performing hypothesis tests. An important question for analysts to consider is: how do we proceed when the response distribution deviates from normality?\nSuch non-Gaussian distributions frequently emerge in practice when dealing with diverse data. For instance:\n\nThe exponential distribution often arises when analyzing waiting times and time-until-event scenarios in survival modeling.\nBernoulli and binomial distributions are prevalent in experiments with single or multiple trials where the occurrence of an event is considered a success (e.g., the number of seropositive samples out of total tested samples).\nPoisson and negative binomial distributions commonly appear in contexts where counts are the variable of interest, such as disease rates or mortality rates.\n\nBroadly, all GLMs have the following form:\n\nThe reponse variable has a clearly defined distribution, which is often fro the exponential family of distributions such as the gaussian, binomial, poisson, etc.\nThere is a linear predictor of the form: \\(\\eta = X\\beta\\), where \\(X\\) is the matrix of predictor (independent) variables, and \\(\\beta\\) is the vector of regression coefficients (the unknowns we are interested in).\nThere exists a link function \\(g(\\cdot)\\) that connects \\(\\mu\\) (the expected value of \\(Y\\)) to the linear predictor, as follows: \\(\\eta = g(\\mu)\\). Here: \\(\\mu = E(Y|X)\\). The link function is sufficient to use if it ensures that the transformed linear predictor \\((X\\beta)\\) will correctly map to the range of the mean of the response variable.1.\n\n1Â See wikipedia for a table on canonical link functions for common response distributions: https://en.wikipedia.org/wiki/Generalized_linear_model\nIn this article, we will use an identity link function for gaussian models, the logit(expit) link function for binomial response models, and the log(exp) link function for the poisson and exponential response models.\n\n\nThe conditional mean of the response(dependent) variable \\(Y\\) depends on the independent variables via:\n\n\\[E(Y | X) = \\mu = g^{-1}(X\\beta)\\]\n\n\nSince: \\(\\eta = g(\\mu)\\), then the inverse becomes: \\(\\mu = g^{-1}(\\eta)\\)\n\nIn this article, I delve into the derivation of GLMs, using a first-principles approach rooted in likelihood theory. It may be beneficial to refresh your understanding of likelihood concepts before proceeding further.\nWhile most sources will emphasize approaching GLMs from an exponential family models approach; here, we will use simple likelihood functions of the assumed response distributions to estimate the parameters of the models given data. Simulated datasets will be used for the purpose of illustration. All analysis will rely on the optim function from R package stats2 for numerical optimization.2Â R Core Team (2023). R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing, Vienna, Austria.https://www.R-project.org/.\n\n\nContinuous data: the gaussian distribution\nWe start with an illustration for the gaussian distribution. Recall: the probability density function of the gaussian distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\) is written as:\n\\[f(x|\\mu,\\sigma^2) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}\\] Given a random sample from \\(y\\) of independent and identically distributed values3 \\((y_1, ..., y_n)\\) the likelihood function is given by:3Â The i.i.d assumption is useful as it greatly simplifies computations using likelihoods i.e.Â joint probabilities become products over marginals\n\\[L(\\mu, \\sigma^2 | y) = \\prod_{i=1}^n f(y_i|\\mu,\\sigma^2) = \\prod_{i=1}^n \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{1}{2}\\left(\\frac{y_i-\\mu}{\\sigma}\\right)^2}\\]\nExpanding the product, and simplifying gives: \\[L(\\mu, \\sigma^2 | y) = \\left(\\frac{1}{\\sigma\\sqrt{2\\pi}}\\right)^n \\prod_{i=1}^n e^{-\\frac{1}{2}\\left(\\frac{y_i-\\mu}{\\sigma}\\right)^2} \\propto \\prod_{i=1}^n e^{-\\frac{1}{2}\\left(\\frac{y_i-\\mu}{\\sigma}\\right)^2} \\]\n\n\nThe \\(\\left(\\frac{1}{\\sigma\\sqrt{2\\pi}}\\right)^n\\) term does not depend on \\(\\mu\\), so we can treat it as a constant.\nWe can further simplify by taking the logarithm (which is monotonic and doesnât change the location of the maximum):\n\\[\\log L(\\mu | y, \\sigma^2) \\propto -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i-\\mu)^2 \\propto -\\sum_{i=1}^n (y_i-\\mu)^2\\] The above equation gives us the log-likelihood.4 We can either proceed by maximizing the above log-likelihood, or equivalently minimize the negative log-likelihood(MSE in this case). As optim by default handles minimization problems, we therefore proceed (here and in every other part) with minimizing the negative log-likelihood.4Â A keen analyst will realize this is equivalent to the negative mean squared error.\n\n\nThe Negative log-likelihood is: \\(\\sum_{i=1}^n (y_i-\\mu)^2\\)\n\n\n\n\n\n\nThe density of the simulated (Y) gaussian random variable\n\n\n\nThe implementation is as follows:\n\n# Generating the data\nset.seed(123)\n\nxmat &lt;- cbind(rep(1, 1000), matrix(runif(5000, min = .2, max = .7), nrow = 1000))\ntruepars &lt;- matrix(c(.1, .2, .3, .9, .9, 3), nrow=6)\neta &lt;- xmat %*% truepars\ny &lt;- eta + rnorm(1000, mean = 0, sd = 1)\n\n# The negative log-likelihood\nnll_gaussian &lt;- function(par, ...) {\n  par &lt;- matrix(par, ncol = 1)\n  mu &lt;- xmat %*% par\n  sum((y - mu)**2)\n}\n\n# actual optimization: mminimizing the NLL and finding the best pars\nbetas &lt;- optim(\n  par = rep(1, 6),\n  fn = nll_gaussian,\n  method = \"Nelder-Mead\",\n  control = list(maxit = 1e3),\n  xmat = xmat,\n  y = y\n)\n\ndata.frame(\n  actual_beta = (truepars),\n  optim_beta = (betas$par |&gt; round(2))\n)\n\n  actual_beta optim_beta\n1         0.1       0.44\n2         0.2      -0.01\n3         0.3       0.20\n4         0.9       0.92\n5         0.9       0.98\n6         3.0       2.43\n\n\n\n\nCount data: the poisson distribution\nThe poisson distribution is the most commonly used distribution for modelling frequency of occurence of events or counts. In insurance, it is commonly used in modelling the claim frequency for a portfolio of insurance policies, while in epidemiology, it is a useful starting point for modelling disease occurences.\nFor a given random variable \\(Y\\), the probability mass function for a poisson distribution with mean \\(\\lambda\\) is written as:\n\\[f(y|\\lambda) = \\frac{e^{-\\lambda} \\lambda^y}{y!}\\]\nGiven a random sample from \\(Y\\) of independent and identically distributed values \\((y_1, ..., y_n)\\) the likelihood function is given by:\n\\[L(\\lambda | y_i) = \\prod_{i=1}^n f(y_i|\\lambda) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{y_i}}{y_i!}\\]\nExpanding the product, and simplifying gives:\n\\[L(\\lambda | y) = \\frac{e^{-n\\lambda}\\lambda^{\\sum_{i=1}^n y_i}}{\\prod_{i=1}^n y_i!} \\propto e^{-n\\lambda}\\lambda^{\\sum_{i=1}^n y_i}\\]\n\n\nThe \\(\\prod_{i=1}^n y_i!\\) term does not depend on \\(\\lambda\\), so we can treat it as a constant.\nThe corresponding log-likelihood becomes:\n\\[\\log L(\\lambda | y) \\propto -n\\lambda + \\sum_{i=1}^n y_i \\log(\\lambda)\\]\nThe above equation gives us the log-likelihood. We proceed with minimizing the negative log-likelihood.\n\n\nThe Negative log-likelihood is: \\(n\\lambda - \\sum_{i=1}^n y_i \\log(\\lambda)\\)\n\n\n\n\n\n\nThe histogram of the simulated (Y) poisson random variable\n\n\n\nThis is implemented in code as follows:\n\n# Generating the data\nset.seed(123)\n\nxmat &lt;- cbind(rep(1, 1000), matrix(runif(5000, min = .2, max = .7),\n                                  nrow = 1000))\ntruepars &lt;- matrix(c(.1, .2, .3, .9, .9, 3), nrow=6)\neta &lt;- xmat %*% truepars\ny &lt;- rpois(1000, exp(eta))\n\n# The negative log-likelihood\nnll_poisson &lt;- function(par, ...) {\n    initpars = par |&gt; matrix()\n    yhat &lt;- exp(xmat %*% initpars)\n    -sum(y * log(yhat) - yhat)\n}\n\n# actual optimization: minimizing the NLL and finding the best pars\nbetas &lt;- optim(\n    par = rep(1, 6),\n    fn = nll_poisson,\n    method = \"Nelder-Mead\",\n    control = list(maxit = 1e3),\n    xmat = xmat,\n    y = y\n)\n\ndata.frame(\n  actual_beta = (truepars),\n  optim_beta = (betas$par |&gt; round(2))\n)\n\n  actual_beta optim_beta\n1         0.1       0.17\n2         0.2       0.22\n3         0.3       0.28\n4         0.9       0.85\n5         0.9       0.85\n6         3.0       2.98\n\n\n\n\nProportions data: the binomial distribution\nThe bernoulli and binomial distribution are widely used for modeling binary outcomes or proportions. Itâs particularly useful in scenarios where weâre interested in the number of successes in a fixed number of independent trials5. Some common applications include: modeling the number of patients responding to a treatment out of the total number treated.5Â For a single trial, the bernoulli distribution is used, while for multiple trials, the binomial distribution is used\nFor a given random variable \\(Y\\), representing the number of successes in \\(n\\) trials, the probability mass function for a binomial distribution with probability of success \\(p\\) is written as:\n\\[f(y|n,p) = \\binom{n}{y} p^y (1-p)^{n-y}\\]\nGiven a random sample of independent and identically distributed values \\((y_1, ..., y_m)\\), where each \\(y_i\\) represents the number of successes in \\(n_i\\) trials, the likelihood function is given by:\n\\[L(p | y_i, n_i) = \\prod_{i=1}^m f(y_i|n_i,p) = \\prod_{i=1}^m \\binom{n_i}{y_i} p^{y_i} (1-p)^{n_i-y_i}\\]\nExpanding the product and simplifying gives:\n\\[L(p | y, n) = \\left(\\prod_{i=1}^m \\binom{n_i}{y_i}\\right) p^{\\sum_{i=1}^m y_i} (1-p)^{\\sum_{i=1}^m (n_i-y_i)} \\propto p^{\\sum_{i=1}^m y_i} (1-p)^{\\sum_{i=1}^m (n_i-y_i)}\\]\n\n\nThe \\(\\prod_{i=1}^m \\binom{n_i}{y_i}\\) term does not depend on \\(p\\), so we can treat it as a constant, and safely ignore it.\nThe log-likelihood becomes:\n\\[\\log L(p | y, n) \\propto \\sum_{i=1}^m y_i \\log(p) + \\sum_{i=1}^m (n_i-y_i) \\log(1-p)\\]\nThe above equation gives us the log-likelihood. We proceed with minimizing the negative log-likelihood as in the gaussian and poisson case.\n\n\nThe Negative log-likelihood is: \\(-\\sum_{i=1}^m y_i \\log(p) - \\sum_{i=1}^m (n_i-y_i) \\log(1-p)\\)\n\n\n\n\n\n\nThe density of the proportions (P) of successes out of the N trials.\n\n\n\nThis formulation allows for varying numbers of trials across observations, which is common in real-world scenarios. If all observations have the same number of trials, you can simplify by replacing \\(n_i\\) with a constant \\(n\\).\nThe code implementation is as follows:\n\n# Generating the data\nset.seed(123)\n\nxmat &lt;- cbind(rep(1, 1000), matrix(runif(5000, min = .2, max = .7), \n                                   nrow = 1000))\ntruepars &lt;- matrix(c(.1, .2, .3, .9, .9, 3), nrow = 6)\nprobs &lt;- plogis(xmat %*% truepars)\nn &lt;- sample(100:200, 1000, replace = TRUE)\ny &lt;- cbind(n, rbinom(1000, size = n, prob = probs))\n# y here is : a matrix of (trials, successes)\n\n\n# The negative log-likelihood\nnll_binomial &lt;- function(par, ...) {\n  par &lt;- matrix(par, ncol = 1)\n  # Using the logistic function to get probabilities\n  p &lt;- plogis(xmat %*% par)\n  nll &lt;- -sum(y[, 2] * log(p) + (y[, 1] - y[, 2]) * log(1 - p))\n  return(nll)\n}\n# actual optimization: minimizing the NLL and finding the best pars\nbetas &lt;- optim(\n  par = rep(0, 6),\n  fn = nll_binomial,\n  method = \"Nelder-Mead\",\n  control = list(maxit = 1e3),\n  xmat = xmat,\n  y = y\n)\n\ndata.frame(\n  actual_beta = (truepars),\n  optim_beta = (betas$par |&gt; round(2))\n)\n\n  actual_beta optim_beta\n1         0.1       0.56\n2         0.2      -0.27\n3         0.3       0.46\n4         0.9       0.55\n5         0.9       0.90\n6         3.0       2.63\n\n\n\n\nTime-to-Event Data: the exponential Distribution\nThe exponential distribution is fundamental in modeling time-to-event data, particularly in parametric survival analysis. Itâs often used as a starting point in survival modelling due to its simplicity and the constant hazard rate assumption. Common applications include: modeling patient survival times or time until disease recurrence and predicting customer churn or time between purchases.\nFor a given random variable \\(Y\\) representing time-to-event, the probability density function for an exponential distribution with scale parameter \\(\\lambda\\) (which represents the mean time to event) is written as:\n\\[f(y|\\lambda) = \\frac{1}{\\lambda} e^{-y/\\lambda}\\]\nGiven a random sample of independent and identically distributed values \\((y_1, ..., y_n)\\), the likelihood function is given by:\n\\[L(\\lambda | y_i) = \\prod_{i=1}^n f(y_i|\\lambda) = \\prod_{i=1}^n \\frac{1}{\\lambda} e^{-y_i/\\lambda}\\]\nExpanding the product and simplifying gives:\n\\[L(\\lambda | y) = \\lambda^{-n} e^{-\\sum_{i=1}^n y_i/\\lambda}\\]\nThe corresponding log-likelihood becomes:\n\\[\\log L(\\lambda | y) = -n \\log(\\lambda) - \\frac{1}{\\lambda} \\sum_{i=1}^n y_i\\]\nThe above equation gives us the log-likelihood. We proceed with minimizing the negative log-likelihood.\n\n\nThe Negative log-likelihood is: \\(n \\log(\\lambda) + \\frac{1}{\\lambda} \\sum_{i=1}^n y_i\\)\n\n\n\n\n\n\nThe density of the simulated Y: exponentially distributed random variable\n\n\n\nThe code implementation is as follows:\n\n# Generating the data\nset.seed(123)\n\nxmat &lt;- cbind(rep(1, 1000), matrix(runif(5000, min = .2, max = .7), nrow = 1000))\ntruepars &lt;- c(.1, .2, .3, .9, .9, 3)\neta &lt;- xmat %*% truepars\ny &lt;- rexp(1000, rate = 1 / exp(eta))\n\n\n# The negative log-likelihood\nnll_exponential &lt;- function(par, ...) {\n    par &lt;- matrix(par, ncol = 1)\n    lambda &lt;- exp(xmat %*% par)\n    nll &lt;- (log(lambda) + (1/lambda) * y)\n    sum(nll)\n}\n\n# actual optimization: minimizing the NLL and finding the best pars\nbetas &lt;- optim(\n  par = rep(1, 6),\n  fn = nll_exponential,\n  method = \"Nelder-Mead\",\n  control = list(maxit = 1e3),\n  xmat = xmat,\n  y = y\n)\n\ndata.frame(\n  actual_beta = (truepars),\n  optim_beta = (betas$par |&gt; round(2))\n)\n\n  actual_beta optim_beta\n1         0.1       0.57\n2         0.2      -0.01\n3         0.3       0.04\n4         0.9       0.69\n5         0.9       0.93\n6         3.0       2.49\n\n\n\n\n\nConclusion\nIn this article, weâve explored Generalized Linear Models (GLMs) from a first-principles perspective, focusing on likelihood-based derivations for several common response distributions: Gaussian, Poisson, Binomial, and Exponential. By approaching GLMs through direct likelihood functions rather than the traditional exponential family framework, weâve provided an alternative, intuitive understanding of these models. This approach not only deepens our understanding of GLMs but also bridges the gap between statistical theory and practical application.\nWhile this likelihood-based method offers valuable insights, itâs important to note that the exponential family approach to GLMs provides additional theoretical advantages, particularly in unifying the treatment of different distributions. Analysts should be familiar with both perspectives.\nI will attempt to cover the exponential family approach in a second article.\n\n\nsessionInfo()\n\nR version 4.3.2 (2023-10-31)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Ventura 13.6.6\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Africa/Nairobi\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] gghighlight_0.4.1 gridExtra_2.3     purrr_1.0.2       stringr_1.5.1    \n[5] sf_1.0-16         ggplot2_3.5.1     dplyr_1.1.4       pacman_0.5.1     \n\nloaded via a namespace (and not attached):\n [1] tidyr_1.3.1        utf8_1.2.4         generics_0.1.3     rstatix_0.7.2     \n [5] class_7.3-22       KernSmooth_2.23-22 stringi_1.8.4      lattice_0.22-6    \n [9] digest_0.6.35      magrittr_2.0.3     evaluate_0.24.0    grid_4.3.2        \n[13] fastmap_1.2.0      jsonlite_1.8.8     Matrix_1.6-5       backports_1.5.0   \n[17] e1071_1.7-14       DBI_1.2.2          mgcv_1.9-1         fansi_1.0.6       \n[21] scales_1.3.0       abind_1.4-5        cli_3.6.2          rlang_1.1.4       \n[25] units_0.8-5        munsell_0.5.1      splines_4.3.2      withr_3.0.0       \n[29] yaml_2.3.8         tools_4.3.2        ggsignif_0.6.4     colorspace_2.1-0  \n[33] ggpubr_0.6.0       broom_1.0.5        vctrs_0.6.5        R6_2.5.1          \n[37] proxy_0.4-27       lifecycle_1.0.4    classInt_0.4-10    car_3.1-2         \n[41] htmlwidgets_1.6.4  pkgconfig_2.0.3    pillar_1.9.0       gtable_0.3.5      \n[45] glue_1.7.0         Rcpp_1.0.13        xfun_0.45          tibble_3.2.1      \n[49] tidyselect_1.2.1   rstudioapi_0.16.0  knitr_1.47         farver_2.1.2      \n[53] htmltools_0.5.8.1  nlme_3.1-164       carData_3.0-5      rmarkdown_2.27    \n[57] labeling_0.4.3     compiler_4.3.2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stanley Sayianka",
    "section": "",
    "text": "The dogmas of the quiet past are inadequate to the stormy present. The occassion is piled high with difficulty, and we must rise - with the occassion. As our case is new, so we must think anew, and act anew.\n- Abraham Lincolnâs address to the Congress (1862)"
  }
]