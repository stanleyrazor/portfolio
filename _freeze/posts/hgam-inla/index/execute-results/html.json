{
  "hash": "92b3a61f1fd96d459cceb53e333cbf65",
  "result": {
    "markdown": "---\ntitle: \"Hierarchical Generalized Additive Models using INLA\"\nsubtitle: \"HGAMs in INLA\"\nauthor: \"Stanley Sayianka\"\nformat:\n  html:\n    grid:\n      margin-width: 400px\nreference-location: margin\ncitation-location: margin\n\ndate: \"2024-08-16\"\ncategories: [code, stats]\nimage: \"cover-img.svg\"\n---\n\n::: {.cell}\n\n:::\n\n\nImage copyrights: https://m-clark.github.io/generalized-additive-models/introduction.html\n\n\n# GAMs and HGAMs\n\nGeneralized Additive Models (GAMs) are a powerful extension of Generalized Linear Models (GLMs), as they offer the flexibility to incorporate non-linear effects of covariates using smooth functions within the GLM framework.\n\nThe model formulation for a GAM can be expressed as:\n$$E(y_i) = \\mu_i$$\n$$g(\\mu_i) = \\eta_i = \\sum_{k=0}^{p_l} \\beta_kz_{ik} + \\sum_{j=1}^{p_s} s_j(x_{ij})$$\n\n::: column-margin\nThe intercept has been absorbed into the linear predictor, and takes the coefficient for $beta_0$\n\n$y_i$ is the response variable\n\n$\\mu$ is the mean of the response\n\n$g(\\cdot)$ is the link function\n\n$\\eta_i$ is the linear predictor, which in this case is a structural additive predictor.\n\n$\\beta_i$ are the regression coefficients for the linear terms\n\n$s_j(\\cdot)$ are smooth functions of the covariates $x_j$\n\n$p_l$ is the number of covariates modelled using the linear functions, and $p_s$ is the number of covariates modelled using the smooth functions.\n:::\n\nThe primary distinction between GLMs and GAMs lies in the latter's ability to incorporate covariates into the model using smooth functions, which are often non-linear. This extension proves particularly valuable when dealing with covariates that exhibit non-linear relationships with the response variable.\n\nIn the traditional GLM framework, analysts typically attempt to account for non-linearity using power functions, trigonometric functions, polynomials, or predefined splines^[think the `splines::ns` or `splines::bs` in R]. However, these approaches may often fall short in capturing complex relationships. GAMs address this limitation by utilizing a wide range of flexible smooth functions based on various basis expansions^[See the `?mgcv::s` or `smooth.terms{mgcv}`].\n\nThe adaptability of GAMs allows for more accurate modeling of complex, non-linear relationships between predictors and the response variable, potentially leading to improved model fit and predictive performance compared to their linear counterparts. An often helpful use case of GAMs are in incorporating spatial effects into models.\n\nA natural extension to the standard GAM, is to build GAM models, where the smooth relationships between covariates and the response are allowed to vary differently by groups in the data. This is what gives rise to the hierarchical GAMs.\n\n# INLA\n\nThe Integrated Nested Laplace Approximation (INLA) is an approximate method for Bayesian inference that offers a fast and efficient alternative to traditional Markov Chain Monte Carlo (MCMC) techniques. INLA aims to provide fast and accurate approximations of posterior distributions for a wide class of latent Gaussian models. \n\nBriefly, for a response variable $y_i$ belonging to the exponential family, where its mean $\\mu_i$ is linked to a structural additive predictor $\\eta_i$ of the form^[Again, the intercept has been absorbed into the summation of the linear predictor]:\n\n$$\\eta_i = \\sum_{k=0}^{n_\\beta}\\beta_kz_{ki} + \\sum_{j=1}^{n_f}f^{(j)}(u_{ji}) + \\epsilon_i$$\n\n::: column-margin\nHere:\n\n$f^{(j)}(\\cdot)$ are unknown functions of certain covariates\n\n$\\beta_k$ represent the linear effects of certain covariates\n\n$\\epsilon_i$ are the unstructured error terms\n:::\n\nLatent gaussian models are a subset of all bayesian models with the structural additive predictor as the one above, where a gaussian prior is assigned to $\\ f^{(j)}(\\cdot), \\ \\beta_k, \\ and  \\  \\epsilon_i$.\n\nThis approach is particularly well-suited for linear, generalized linear and generalized additive models, even those with spatial, temporal, as well as hierarchical structures. This makes INLA an excellent choice for implementing Hierarchical Generalized Additive Models (HGAMs). INLA's computational efficiency allows for the analysis of complex models and large datasets that might be computationally prohibitive with MCMC methods. \n\nThe `{R-INLA}` package brings this methodology to R. For those interested in a comprehensive yet accessible introduction to INLA and its implementation in R, Virgilio Gómez-Rubio (2020)^[Gómez-Rubio, Virgilio (2020). Bayesian Inference with INLA. Chapman & Hall/CRC Press. Boca Raton, FL.] and  Wang et al. (2018)^[Wang, X., Yue, Y. R., & Faraway, J. J. (2018). Bayesian regression modeling with INLA. Chapman and Hall/CRC.] serve as excellent resources.\n\nIn INLA, the shape of $f(\\cdot)$ , the smooth terms are commonly represented using the following functions: \n\n1. $AR(p)$: Autoregressive models of order p, with p being at most 10.\n\n2. $RW(u)$: Random walk terms. In INLA, currently only random walks of order 1 and 2 are implemented.\n\n3. For irregularly spaced data, Stochastic Partial Differential Equation (SPDE) are implemented in INLA.\n\n\n::: column-margin\n\nFor auto-regressive models:\n\n$$y_t = \\alpha + u_t + \\varepsilon_t, \\quad t = 1, \\ldots, n$$\n\n$$u_1 \\sim N(0, \\tau_u(1-\\rho^2)^{-1})$$\n\n$$u_t = \\rho u_{t-1} + \\epsilon_t, \\quad t = 2, \\ldots, n$$\n\n$$\\varepsilon_t \\sim N(0, \\tau_\\varepsilon), \\quad t = 1, \\ldots, n$$\n\nFor random walk models (here order: 1) :\n\n$$y_t = \\alpha + u_t + \\varepsilon_t, \\quad t = 1, \\ldots, n$$\n\n$$u_t - u_{t-1} \\sim N(0, \\tau_u), \\quad t = 2, \\ldots, n$$\n\n$$\\varepsilon_t \\sim N(0, \\tau_\\varepsilon), \\quad t = 1, \\ldots, n$$\n:::\n\nMore information on the derivations and motivations for using the above can be found in Virgilio Gómez-Rubio (2020) Chpt. 9^[https://becarioprecario.bitbucket.io/inla-gitbook/ch-smoothing.html].\n\n\n\n# Models\n\nIn this blog, I will try to replicate the models described in this paper using the INLA methodology:\n\nPedersen EJ, Miller DL, Simpson GL, Ross N. 2019. Hierarchical generalized additive models in ecology: an introduction with mgcv. PeerJ 7:e6876 DOI 10.7717/peerj.6876\n\nThis excellent paper, provides an overview of the connection between hierarchical GLMs and GAMs, their similarities in terms of how they fit highly variable models to data, especially those involving group specific terms (intercepts and slopes), and their applications to ecology.\n\nA key advantage of HGAMs is the ability to provide sensible estimates, in cases where each group in the data does not have sufficient data points (building seperate models for different groups could lead to noisy predictions), the hierarchical component also allows us to build models that are able to give better predictions in different groups, as well as overall-y on average (building models while ignoring any hierarchy/grouping present in the data leads to poor predictions for any group).\n\nThe specific models considered in the paper are classified into the following:\n\n1. Model G: A single common smoother for all observations i.e a global smoother.\n\n2. Model GS: A global smoother plus group-level smoothers that have the same wiggliness.\n\n3. Model GI: A global smoother plus group-level smoothers with differing wiggliness.\n\n4. Model S: Group-specific smoothers having same wiggliness. This model does not contain a global smoother.\n\n5. Model I: Group-specific smoothers with different wiggliness. This model does not contain a global smoother.\n\n\n## Dataset 1: `datasets::CO2`\n\nAn experiment on the cold tolerance of a particular grass species, was conducted to measure carbon dioxide ($CO_2$) absorption in plants from two different regions. Twelve plants were used in the study: six sourced from Quebec and six from Mississippi. The researchers assessed the plants' $CO_2$ uptake at various concentrations of atmospheric carbon dioxide. As part of the experimental design, half of the plant specimens were subjected to overnight chilling prior to data collection. This resulted in a total of 84 observations.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n|Variable  |Descriptions                                                                                         |\n|:---------|:----------------------------------------------------------------------------------------------------|\n|Plant     |an ordered factor with levels Qn1 < Qn2 < Qn3 < ... < Mc1 giving a unique identifier for each plant. |\n|Type      |a factor with levels Quebec Mississippi giving the origin of the plant                               |\n|Treatment |a factor with levels nonchilled chilled                                                              |\n|conc      |a numeric vector of ambient carbon dioxide concentrations (mL/L).                                    |\n|uptake    |a numeric vector of carbon dioxide uptake rates                                                      |\n:::\n:::\n\n\nFor the HGAM paper, a slight modification was done to include a variable *Plant_uo*, which is an unordered factor variable, and is a replication of the *Plant* variable included in the dataset. The response variable is the log of `Uptake`.\n\n### Model G: Global smoother\n\nIn this first model, we include a random intercept for the plant identifiers, and use the RW2 smoother for log concentration.^[In the paper, log-uptake is modelled using two smoothers: a TPRS(`smooth.construct.tp.smooth.spec {mgcv}`) of log-concentration (conc), and a random effect smooth(`smooth.construct.re.smooth.spec {mgcv}`) for the plant identifier (plant_uo).]\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# mgcv model\ng_mgcv <- gam(log_uptake ~ s(log_conc, k=5, bs=\"tp\") + s(plant_uo, k=12, bs=\"re\"),\n    data=d1, method=\"REML\", family=\"gaussian\")\n\n# inla model\ng_inla <- inla(log_uptake ~ f(log_conc, model = \"rw2\") + f(plant_uo, model = \"iid\"),\n           family = \"gaussian\", data = d1,\n           control.compute = list(dic = T, waic = T))\n\nd1 |>\n  mutate(\n    mgcv = predict(g_mgcv, type = 'response') |> as.numeric() |> exp(),\n    inla = g_inla$summary.fitted.values$mean |> exp()\n  ) |>\n  ggplot() + \n  geom_point(aes(x = conc, y = uptake)) + \n  geom_line(aes(x = conc, y = mgcv), col = 'red', lty = 'dashed') + \n  geom_line(aes(x = conc, y = inla), col = 'black') + \n  facet_wrap( ~ plant_uo, scales = 'free_y') +\n  labs(\n    title = \"Model G: Global Smoothers\",\n    x = expression('CO'[2]~''~'Concentration (mL L'^-1*')'),\n    y = expression('CO'[2]~'Uptake (µmol m'^-2~')'),\n        caption = \"The black fitted line is the INLA model, whereas, the red dashed line is the MGCV GAM model\"\n  ) +\n  theme_bw() + \n  my_theme\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/g-1.png){width=960}\n:::\n:::\n\n\nFrom the fitted model, the global smoothing is adequate, although it under estimates the log-uptake in some plant species such as Qn2, and over estimates the log-uptake in plant species such as Mc2 and Mc3. It is clear that the RW2 and the TPRS model have similar predictions.\n\n### Model GS: A global smoother and group-level smoothers with same wiggliness\n\nThis second model resembles a mixed effect model with varying slopes. In the model - on top of the global smoother, each group is allowed to have its own smoother for a given variable, however the smoothers are estimated using one parameter for all groups (yielding the same wiggliness), and are penalized towards zero. The mgcv implementation of this is in the smooth function `fs` for factor-smoother interaction. This leads to a differing shape in the functional response of each group, as shown below.\n\nThe `R-INLA` implementation, will rely on using the `replicate`` function^[Note that in mgcv, we do not include the random intercepts smooth, as they are included in the factor-smooth interactions.].\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# mgcv model\ngs_mgcv <- gam(log_uptake ~ \n                 s(log_conc, k = 5, m = 2) + \n                 s(log_conc, plant_uo, k = 5, bs = \"fs\", m = 2),\n    data=d1, method=\"REML\")\n\n# inla model\ngs_inla <- inla(log_uptake ~ \n                  f(plant_uo, model=\"iid\") +\n                  f(log_conc_global, model = 'rw2') + \n                  f(log_conc, model = \"rw2\", replicate = plant_uo_id),\n                family = \"gaussian\",\n                data = d1 |>\n                  mutate(plant_uo_id = as.integer(plant_uo),\n                         log_conc_global = log_conc),\n                control.compute = list(dic = T, waic = T))\n\n\nd1 |>\n  mutate(\n    mgcv = predict(gs_mgcv, type = 'response') |> as.numeric() |> exp(),\n    inla = gs_inla$summary.fitted.values$mean |> exp(),\n  ) |>\n  ggplot() + \n  geom_point(aes(x = conc, y = uptake)) + \n  geom_line(aes(x = conc, y = mgcv), col = 'red', lty='dashed') + \n  geom_line(aes(x = conc, y = inla), col = 'black') + \n  facet_wrap(~plant_uo, scales = 'free_y') +\n  labs(\n    title = \"Model GS: Global Smoothers + group smoothers with same wiggliness\",\n    caption = \"The black fitted line is the INLA model, whereas, the red dashed line is the MGCV GAM model\",\n    x = expression('CO'[2]~''~'Concentration (mL L'^-1*')'),\n    y = expression('CO'[2]~'Uptake (µmol m'^-2~')'))+\n  theme_bw() + \n  my_theme\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/gs-1.png){width=960}\n:::\n:::\n\n\nThe above fitted lines show an improvement over the model G, and the INLA and mgcv predictions are also close.\n\n### Model GI:  A global smoother plus group-level smoothers with differing wiggliness\n\nThis is an extension of the G model above, where each group smoother is allowed to have its own parameters, which yields a model where the smoothers have differing wiggliness. This is an important extension if there is reason to believe that each group smooth differs in how wiggly it is.\n\nThis is achieved in INLA using the `group` argument when specifying functions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# mgcv model\ngi_mgcv <- gam(log_uptake ~ \n                 s(plant_uo, bs = 're', k = 12) + \n                 s(log_conc, k = 5, m = 2, bs = 'tp') + \n                 s(log_conc, k = 5, m = 1, bs = \"tp\", by = plant_uo),\n               data=d1, method=\"REML\")\n\n# inla model\ngi_inla <- inla(log_uptake ~ \n                  f(plant_uo, model=\"iid\") +\n                   f(log_conc_global, model = \"rw2\") + \n                  f(log_conc, model = \"rw2\", group = plant_uo_id),\n                family = \"gaussian\",\n                data = d1 |>\n                  mutate(plant_uo_id = as.integer(plant_uo),\n                         log_conc_global = log_conc),\n                control.compute = list(dic = T, waic = T))\n\nd1 |>\n  mutate(\n    mgcv = predict(gi_mgcv, type = 'response') |> as.numeric() |> exp(),\n    inla = gi_inla$summary.fitted.values$mean |> exp(),\n  ) |>\n  ggplot() + \n  geom_point(aes(x = conc, y = uptake)) + \n  geom_line(aes(x = conc, y = mgcv), col = 'red', lty='dashed') + \n  geom_line(aes(x = conc, y = inla), col = 'black') +\n  facet_wrap(~plant_uo, scales = 'free_y') +\n  labs(\n    title = \"Model GI: Global Smoothers + group smoothers with differing wiggliness\",\n    caption = \"The black fitted line is the INLA model, whereas, the red dashed line is the MGCV GAM model\",\n    x = expression('CO'[2]~''~'Concentration (mL L'^-1*')'),\n    y = expression('CO'[2]~'Uptake (µmol m'^-2~')'))+\n  theme_bw() + \n  my_theme\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/gi-1.png){width=960}\n:::\n:::\n\n\nThere isn't much difference as compared to the GS model above, however, the GI model and mgcv HGAM model have close predictions. The GI model is an improvement over the model G.\n\n### Models without global smoothers (Model S and I)\n\nIn this section, we focus on implementing hierarchical generalized additive models (HGAMs) without global smooths. By omitting the global smoother, we remove the constraint that penalizes deviations of group-level smooths from a common smooth This is particularly important when we believe that the group-level smooths may not align with or may differ significantly from a common global pattern.\n\nModel S is an instance of model GS above, without the global smooth, and model I is an instance of model GI above, without the global smooth.\n\nA drawdown of using models without global smooths (model S and I) is that, in instances where the number of data points is small in each group, then the results from the models S & I will be more variable than those from model GS and GI.  This increased variability stems from the fact that group-specific smooths in models S & I rely solely on within-group data, without the stabilizing influence of a global smooth or leveraged information across all groups.\n\nAnother drawdown is: without global smooths, it is not possible to predict the response variables for groups not included in the training set, and it is also not possible to compute an average effect of the response variable.\n\n#### Model S\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# mgcv model\ns_mgcv <- gam(log_uptake ~ \n                 s(log_conc, plant_uo, k = 5, bs = \"fs\", m = 2),\n    data=d1, method=\"REML\")\n\n# inla model\ns_inla <- inla(log_uptake ~ \n                  f(plant_uo, model=\"iid\") +\n                  f(log_conc, model = \"rw2\", replicate = plant_uo_id),\n                family = \"gaussian\",\n                data = d1 |>\n                  mutate(plant_uo_id = as.integer(plant_uo)),\n               control.compute = list(dic = T, waic = T))\n\n\nd1 |>\n  mutate(\n    mgcv = predict(s_mgcv, type = 'response') |> as.numeric() |> exp(),\n    inla = s_inla$summary.fitted.values$mean |> exp(),\n  ) |>\n  ggplot() + \n  geom_point(aes(x = conc, y = uptake)) + \n  geom_line(aes(x = conc, y = mgcv), col = 'red', lty='dashed') + \n  geom_line(aes(x = conc, y = inla), col = 'black') + \n  facet_wrap(~plant_uo, scales = 'free_y') +\n  labs(\n    title = \"Model S: Group smoothers with same wiggliness\",\n    caption = \"The black fitted line is the INLA model, whereas, the red dashed line is the MGCV GAM model\",\n    x = expression('CO'[2]~''~'Concentration (mL L'^-1*')'),\n    y = expression('CO'[2]~'Uptake (µmol m'^-2~')'))+\n  theme_bw() + \n  my_theme\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/s-1.png){width=960}\n:::\n:::\n\n\nThe predictions from the INLA model, are quite close to the actual values for the response variable.\n\n#### Model I\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# mgcv model\ni_mgcv <- gam(log_uptake ~ \n                 s(plant_uo, bs = 're', k = 12) + \n                 s(log_conc, k = 5, m = 1, bs = \"tp\", by = plant_uo),\n               data=d1, method=\"REML\")\n\n# inla model\ni_inla <- inla(log_uptake ~ \n                  f(plant_uo, model=\"iid\") +\n                  f(log_conc, model = \"rw2\", group = plant_uo_id),\n                family = \"gaussian\",\n                data = d1 |>\n                  mutate(plant_uo_id = as.integer(plant_uo)),\n               control.compute = list(dic = T, waic = T))\n\nd1 |>\n  mutate(\n    mgcv = predict(i_mgcv, type = 'response') |> as.numeric() |> exp(),\n    inla = i_inla$summary.fitted.values$mean |> exp(),\n  ) |>\n  ggplot() + \n  geom_point(aes(x = conc, y = uptake)) + \n  geom_line(aes(x = conc, y = mgcv), col = 'red', lty='dashed') + \n  geom_line(aes(x = conc, y = inla), col = 'black') +\n  facet_wrap(~plant_uo, scales = 'free_y') +\n  labs(\n    title = \"Model I: Group smoothers with differing wiggliness\",\n    caption = \"The black fitted line is the INLA model, whereas, the red dashed line is the MGCV GAM model\",\n    x = expression('CO'[2]~''~'Concentration (mL L'^-1*')'),\n    y = expression('CO'[2]~'Uptake (µmol m'^-2~')'))+\n  theme_bw() + \n  my_theme\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/i-1.png){width=960}\n:::\n:::\n\n\nThe predictions from the INLA model are closer to those of the MGCV HGAMs for certain groups.\n\n# Comparisons\n\nIn this section, we compare the fitted models (G, S, I, GS, GI) using the Deviance Information Criterion, as shown below:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.frame(\n  model = c(\"Model G\", \"Model S\", \"Model I\", \"Model GS\", \"Model GI\"),\n  DIC = list(g_inla, s_inla, i_inla, gs_inla, gi_inla) |> lapply(FUN = \\(x) x$dic$dic) |> unlist(),\n  WAIC = list(g_inla, s_inla, i_inla, gs_inla, gi_inla) |> lapply(FUN = \\(x) x$waic$waic) |> unlist()\n) |>\n  knitr::kable()\n```\n\n::: {.cell-output-display}\n|model    |       DIC|      WAIC|\n|:--------|---------:|---------:|\n|Model G  | -116.1953| -111.2939|\n|Model S  | -444.0115| -448.7367|\n|Model I  | -215.8004| -214.5594|\n|Model GS | -210.8557| -209.1680|\n|Model GI | -209.9247| -208.6807|\n:::\n:::\n\n\n::: column-margin\nIn the HGAM paper, the models from best fitting to worst are: I > S > GI > GS > G, with model I and S being so close in AIC values.\n:::\n\nFrom the table above, model S seems to be the best fitting model, as indicated by the lowest DIC & WAIC values.\n\nThe order of fit, from best fitting to worst is: S > I > GS > GI > G. However, the performance of model GS and GI are so close.\n\nThis indicates that, for the CO2 dataset:\n\n1. There is a compelling reason for including group level smoothing. This is because the model without group level smooths (model G) performs the worst as compared to the rest of the models.\n\n2. There is no need for independent group specific smooths (model GI), as the performance gain from using common/shared group smooths (model GS) is minimal.\n\n3. There is no need for a global smooth, as model S & I have better performance as compared to all models which included a global smooth\n\n4. As model S has better performance than I, this implies that there is a common smooth structure in how uptake is affected by changes in concentration across all plant specimens, and that the additional complexity of plant-specific smooth terms is not justified by the data.\n\n# References\n\nBooks\n\n- Gómez-Rubio, V. (2020). *Bayesian inference with INLA*. Chapman and Hall/CRC.\n\n- Wang, X., Yue, Y. R., & Faraway, J. J. (2018). *Bayesian regression modeling with INLA*. Chapman and Hall/CRC.\n\n- Wickham, H. (2016). *ggplot2: Elegant Graphics for Data Analysis*. Springer-Verlag New York.\n\n- Wood, S. N. (2017). *Generalized Additive Models: An Introduction with R* (2nd ed.). Chapman and Hall/CRC.\n\nArticles\n\n- Pedersen, E. J., Miller, D. L., Simpson, G. L., & Ross, N. (2019). Hierarchical generalized additive models in ecology: An introduction with mgcv. *PeerJ*, 7, e6876.\n\n- Rue, H., Martino, S., & Chopin, N. (2009). Approximate Bayesian inference for latent Gaussian models using integrated nested Laplace approximations (with discussion). *Journal of the Royal Statistical Society: Series B (Statistical Methodology)*, 71(2), 319–392.\n\n- Wood, S. N. (2003). Thin-plate regression splines. *Journal of the Royal Statistical Society: Series B (Statistical Methodology)*, 65(1), 95–114.\n\nSoftware\n\n- R Core Team (2023). *R: A Language and Environment for Statistical Computing*. R Foundation for Statistical Computing, Vienna, Austria. Retrieved from <https://www.R-project.org/>\n\n---\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nR version 4.3.2 (2023-10-31)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Ventura 13.6.6\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Africa/Nairobi\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] mgcv_1.9-1    nlme_3.1-164  INLA_24.02.09 sp_2.1-3      Matrix_1.6-5 \n[6] ggplot2_3.5.1 dplyr_1.1.4   pacman_0.5.1 \n\nloaded via a namespace (and not attached):\n [1] utf8_1.2.4         generics_0.1.3     class_7.3-22       fmesher_0.1.5     \n [5] KernSmooth_2.23-22 lattice_0.22-6     digest_0.6.35      magrittr_2.0.3    \n [9] evaluate_0.24.0    grid_4.3.2         fastmap_1.2.0      jsonlite_1.8.8    \n[13] e1071_1.7-14       DBI_1.2.2          fansi_1.0.6        scales_1.3.0      \n[17] cli_3.6.2          rlang_1.1.4        units_0.8-5        munsell_0.5.1     \n[21] splines_4.3.2      withr_3.0.0        yaml_2.3.8         tools_4.3.2       \n[25] parallel_4.3.2     MatrixModels_0.5-3 colorspace_2.1-0   vctrs_0.6.5       \n[29] R6_2.5.1           proxy_0.4-27       lifecycle_1.0.4    classInt_0.4-10   \n[33] htmlwidgets_1.6.4  pkgconfig_2.0.3    pillar_1.9.0       gtable_0.3.5      \n[37] glue_1.7.0         Rcpp_1.0.13        sf_1.0-16          xfun_0.45         \n[41] tibble_3.2.1       tidyselect_1.2.1   rstudioapi_0.16.0  knitr_1.47        \n[45] farver_2.1.2       htmltools_0.5.8.1  rmarkdown_2.27     labeling_0.4.3    \n[49] compiler_4.3.2    \n```\n:::\n:::",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}